{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b9b2c86f",
      "metadata": {
        "id": "b9b2c86f"
      },
      "source": [
        "# Desenvolvendo Modelo de Machine Learning\n",
        "\n",
        "Agora que temos o _dataset_ com as imagens e as labels podemos começar a desenvovler o modelo. Começamos fazendo pesquisa em relação a outros modelos de ML que façam algo similar e que tecnologias são mais usadas atualmente em visão computacional. Após a pesquisa descobrimos o Detectron2, ele é um framework código aberto desenvolvido pelo Facebook AI Research (FAIR) que fornece uma plataforma de alto desempenho e fácil de usar para a pesquisa de detecção e segmentação de objetos.\n",
        "\n",
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n",
        "O Detectron2 permite a implementação fácil e eficiente de algoritmos de detecção de objetos, incluindo Mask R-CNN. Mask R-CNN é um algoritmo de visão computacional, uma extensão do Faster R-CNN, um algoritmo de detecção de objetos. Enquanto Faster R-CNN é projetado para identificar a localização dos objetos em uma imagem e classificá-los em categorias, Mask R-CNN vai um passo além e adiciona uma terceira tarefa: gerar uma máscara de segmentação para cada instância do objeto detectado. A máscara é basicamente um mapa binário que indica os pixels na imagem que pertencem ao objeto.\n",
        "\n",
        "Mask R-CNN é bastante útil para tarefas de detecção e _object segmentation_, porque pode fornecer informações mais detalhadas sobre a forma e a localização do objeto. Na tarefa de detecção de telhados e paines solares em imagens de satélite, por exemplo, o Mask R-CNN pode ajudar a identificar a localização exata e a forma de cada telhado, o que pode ser muito útil para a aplicação que queremos, de poder fazer uma estimativa de tamanho.\n",
        "\n",
        "A segmentação de instâncias (ou _instance segmentation_) é uma tarefa de visão computacional que envolve não apenas a identificação e classificação de objetos em uma imagem (_object detection_), mas também a determinação exata dos pixels que pertencem a cada objeto identificado. Então enquanto a detecção de objetos pode identificar e classificar vários objetos em uma imagem e fornecer uma caixa delimitadora ao redor de cada objeto, a _instance segmentation_ vai além e fornece uma máscara binária para cada objeto, representando exatamente a forma do objeto, o que é importatne para nosso objetivo ja que queremos a forma exata do telhado, e não apenas identifica-los em uma imagem.\n",
        "\n",
        "Por isso usamos o Mask R-CNN, que é um modelo de rede neural convolucional (CNN) que foi desenvolvido para tarefas de detecção e segmentação de objetos em imagens, _instance segmentation_. Ele é uma extensão do modelo Faster R-CNN, que é usado para detecção de objetos. O Mask R-CNN adiciona uma etapa adicional ao Faster R-CNN para gerar máscaras de segmentação precisas para cada objeto detectado na imagem. Isso permite que o modelo localize objetos com mais precisão e segmente-os com mais precisão do que o Faster R-CNN, fazendo-o mais adequado para nossa aplicação.\n",
        "\n",
        "O Mask R-CNN é amplamente utilizado em aplicações de visão computacional, e esta inlcuido no _framework_ Detectron2, que por sua vez, é uma plataforma que permite a implementação eficiente de algoritmos de detecção de objetos, incluindo Mask R-CNN. Ele fornece uma infraestrutura robusta e flexível para treinar e executar modelos Mask R-CNN, com suporte para muitas variantes e extensões, e é baseado no Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53884b4f",
      "metadata": {
        "id": "53884b4f"
      },
      "outputs": [],
      "source": [
        "#Aqui foram apenas algumas configurações e testes ↓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d2cca2",
      "metadata": {
        "scrolled": true,
        "id": "e1d2cca2",
        "outputId": "da66f1c1-a596-4b68-f134-dafecd979d45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/tiberio/anaconda3/bin/python\n",
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n",
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2cd969",
      "metadata": {
        "id": "7e2cd969",
        "outputId": "b28e240f-f068-4d9e-937e-7715fa376212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS is available. Device: mps\n"
          ]
        }
      ],
      "source": [
        "# Metal Performance Shaders\n",
        "import torch\n",
        "\n",
        "if not torch.backends.mps.is_available():\n",
        "    if not torch.backends.mps.is_built():\n",
        "        print(\"MPS not available because the current PyTorch install was not \"\n",
        "              \"built with MPS enabled.\")\n",
        "    else:\n",
        "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
        "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
        "else:\n",
        "    mps_device = torch.device(\"mps\")\n",
        "    print(\"MPS is available. Device:\", mps_device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d1f715",
      "metadata": {
        "id": "78d1f715",
        "outputId": "41fd4f37-75c9-4999-c802-de268d185bb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deletado com sucesso!\n",
            "Deletado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "#Deletador2000\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "#remove 'satellite_dataset' de DatasetCatalog e MetadataCatalog\n",
        "if \"satellite_dataset\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"satellite_dataset\")\n",
        "    print (\"Deletado com sucesso!\")\n",
        "if \"satellite_dataset\" in MetadataCatalog.list():\n",
        "    MetadataCatalog.remove(\"satellite_dataset\")\n",
        "    print (\"Deletado com sucesso!\")\n",
        "else:\n",
        "    print(\"Não tinha nada...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ff2b1b",
      "metadata": {
        "id": "d4ff2b1b"
      },
      "source": [
        "↓ Primeira versão (funcional) do modelo que batizamos de **SolEngine**. Após instalar o PyTorch e o [Detectron2](https://github.com/facebookresearch/detectron2) nos damos ao Detectron o _path_ para a pasta com nossas imagens para treino e o arquivo JSON no formato COCO. O COCO (Common Objects in Context) é um conjunto de dados de referência muito utilizado em tarefas de visão computacional, como detecção e segmentação de objetos, nesse arquivo contem a lsita de todas as imagens e as _labels_ que fizemos no Label Studio. Depois importamos os modelos do Detectron e escolhemos o que achamos que melhor encaixaria em nossa necessidade, o Mask R-CNN 50 FPN 3x. Depois das importações podemos passar para importante etapa de configurar os hiperparametros. Hiperparâmetros em machine learning são configurações ajustáveis que determinam a estrutura e o desempenho de um algoritmo de aprendizado de máquina. Diferentes dos parâmetros do modelo que o algoritmo aprende sozinho durante o treinamento, os hiperparâmetros devem ser definidos antes do treinamento e geralmente permanecem constantes durante todo o processo. A escolha dos hiperparâmetros pode ter um impacto significativo no desempenho de um modelo de aprendizado de máquina, e encontrar a combinação correta geralmente envolve um processo de experimentação e ajuste, conhecido como otimização de hiperparâmetros. Isso pode ser feito de várias maneiras, incluindo busca em grade (grid search), busca aleatória (random search) e otimização bayesiana, entre outros.\n",
        "\n",
        "Algum dos hiperparâmetros que usamos foram:\n",
        "\n",
        "- **Taxa de aprendizado (Learning Rate):** controla o quanto os parâmetros do modelo são ajustados a cada iteração do algoritmo. Uma taxa de aprendizado muito alta pode fazer com que o algoritmo passe pela solução ideal, enquanto uma taxa de aprendizado muito baixa pode tornar o treinamento desnecessariamente lento. É uma das partes mais importantes do treinamento de um modelo de aprendizado de máquina, pois controla a taxa na qual o modelo é atualizado em resposta ao erro que está vendo. Usamos um valor de 0.02 como um ponto de partida considerado razoável, mas pode (e foi) ser ajustado para otimizar a convergência do treinamento.\n",
        "\n",
        "- **Tamanho do lote (Batch Size):** é o número de exemplos de treinamento usados em uma iteração do algoritmo de otimização. Refere-se ao número de amostras que serão propagadas através da rede ao mesmo tempo. Devido ao baixa resolução das imagens e a grande quantidade de RAM disponivel fomos gradativamente aumentando esse numero com nossos testes.\n",
        "\n",
        "- **Número de épocas (Number of Epochs)**: define quantas vezes o algoritmo passará pelo conjunto de treinamento inteiro. Mais épocas podem permitir que o modelo aprenda melhor, mas também pode levar a um _overfit_ se o número for muito alto. O número de epochs não é diretamente especificado nas configurações do Detectron2. No entanto, o conceito de epochs é indiretamente controlado pelo parâmetro `cfg.SOLVER.MAX_ITER`. O número de iterações de treinamento precisa ser suficiente para permitir que o modelo convirja, mas não tão alto a ponto de causar _overfit_. O valor ideal pode variar dependendo do tamanho do conjunto de dados de treinamento e da complexidade do problema.\n",
        "\n",
        "- **RoI Head Batch Size Per Image**: É um parâmetro do modelo no Detectron2 que controla o número de amostras de regiões de interesse (RoIs) que são processadas por imagem em cada mini-lote (batch) durante o treinamento da rede de cabeça (head network) do modelo. As RoIs são regiões propostas pelo modelo que podem conter objetos de interesse, e a rede de cabeça é responsável por classificar e regredir essas RoIs para gerar as previsões finais do modelo. Em áreas urbanas densas, pode haver muitos telhados por imagem, então um valor relativamente alto como 128 pode ser necessário.\n",
        "\n",
        "\n",
        "Atraves de pesquisa de modelos similares e ajuda do ChatGPT chegamos a os parametros que usamos abaixo para a primeira versão funcional e estavel do nosso modelo, levando em consideração a aplicação do modelo, o dataset e o hardware da maquina.\n",
        "\n",
        "Começamos de forma mais modesta até encotrar um modelo funcional (abaixo), depois tentamos tecnicas diferentes, como iremos mostrar depois.\n",
        "\n",
        "O `cfg.MODEL.DEVICE = \"cpu\"` é uma declaração do detectron2 para que o PyTorch trabalhe apenas com a CPU e ignore a GPU. Isso foi devido ao fato desse modelo ter sido treinado em um Mac, e o Detectron2 foi feito para ter aceleração com GPUs usando CUDA cores, que são especificos de placas da NVIDIA. Esse comando faz com que o programa funcione em sistemas que não contam com GPUs da NVIDIA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f35a1e",
      "metadata": {
        "id": "e8f35a1e",
        "outputId": "4d223c95-5222-481b-edec-dc7257b9b3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 11:49:28 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/24 11:49:28 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[05/24 11:49:28 d2.data.datasets.coco]: \u001b[0mLoaded 204 images in COCO format from ./labeled_sat_images/result.json\n",
            "\u001b[32m[05/24 11:49:28 d2.data.build]: \u001b[0mRemoved 1 images with no usable annotations. 203 images left.\n",
            "\u001b[32m[05/24 11:49:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[05/24 11:49:28 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[05/24 11:49:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[05/24 11:49:28 d2.data.common]: \u001b[0mSerializing 203 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[05/24 11:49:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.54 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[05/24 11:49:28 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[05/24 11:49:28 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 11:49:28 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tiberio/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using TensorFlow backend.\n",
            "/Users/tiberio/anaconda3/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 11:50:35 d2.utils.events]: \u001b[0m eta: 1:42:48  iter: 19  total_loss: 2.558  loss_cls: 0.8733  loss_box_reg: 0.7599  loss_mask: 0.6861  loss_rpn_cls: 0.1801  loss_rpn_loc: 0.07679    time: 2.9963  last_time: 2.4417  data_time: 0.0645  last_data_time: 0.0012   lr: 0.00039962  \n",
            "\u001b[32m[05/24 11:51:36 d2.utils.events]: \u001b[0m eta: 1:41:46  iter: 39  total_loss: 2.075  loss_cls: 0.5236  loss_box_reg: 0.8161  loss_mask: 0.535  loss_rpn_cls: 0.09698  loss_rpn_loc: 0.08342    time: 3.0297  last_time: 3.2505  data_time: 0.0010  last_data_time: 0.0010   lr: 0.00079922  \n",
            "\u001b[32m[05/24 11:52:31 d2.utils.events]: \u001b[0m eta: 1:37:24  iter: 59  total_loss: 1.625  loss_cls: 0.4084  loss_box_reg: 0.6943  loss_mask: 0.3855  loss_rpn_cls: 0.08105  loss_rpn_loc: 0.07313    time: 2.9435  last_time: 2.2480  data_time: 0.0009  last_data_time: 0.0008   lr: 0.0011988  \n",
            "\u001b[32m[05/24 11:53:29 d2.utils.events]: \u001b[0m eta: 1:35:42  iter: 79  total_loss: 1.485  loss_cls: 0.3688  loss_box_reg: 0.6227  loss_mask: 0.327  loss_rpn_cls: 0.07557  loss_rpn_loc: 0.05603    time: 2.9274  last_time: 2.6160  data_time: 0.0010  last_data_time: 0.0010   lr: 0.0015984  \n",
            "\u001b[32m[05/24 11:54:28 d2.utils.events]: \u001b[0m eta: 1:35:23  iter: 99  total_loss: 1.551  loss_cls: 0.4363  loss_box_reg: 0.55  loss_mask: 0.3171  loss_rpn_cls: 0.08031  loss_rpn_loc: 0.08418    time: 2.9370  last_time: 2.8876  data_time: 0.0010  last_data_time: 0.0013   lr: 0.001998  \n",
            "\u001b[32m[05/24 11:55:28 d2.utils.events]: \u001b[0m eta: 1:34:51  iter: 119  total_loss: 1.41  loss_cls: 0.4055  loss_box_reg: 0.5956  loss_mask: 0.2972  loss_rpn_cls: 0.05811  loss_rpn_loc: 0.07273    time: 2.9406  last_time: 2.7930  data_time: 0.0009  last_data_time: 0.0010   lr: 0.0023976  \n",
            "\u001b[32m[05/24 11:56:27 d2.utils.events]: \u001b[0m eta: 1:35:02  iter: 139  total_loss: 1.294  loss_cls: 0.357  loss_box_reg: 0.5196  loss_mask: 0.2816  loss_rpn_cls: 0.05307  loss_rpn_loc: 0.05781    time: 2.9431  last_time: 3.1097  data_time: 0.0010  last_data_time: 0.0011   lr: 0.0027972  \n",
            "\u001b[32m[05/24 11:57:26 d2.utils.events]: \u001b[0m eta: 1:34:23  iter: 159  total_loss: 1.497  loss_cls: 0.4145  loss_box_reg: 0.6245  loss_mask: 0.3378  loss_rpn_cls: 0.06589  loss_rpn_loc: 0.06975    time: 2.9427  last_time: 2.5973  data_time: 0.0009  last_data_time: 0.0008   lr: 0.0031968  \n",
            "\u001b[32m[05/24 11:58:26 d2.utils.events]: \u001b[0m eta: 1:33:40  iter: 179  total_loss: 1.465  loss_cls: 0.3717  loss_box_reg: 0.5716  loss_mask: 0.3229  loss_rpn_cls: 0.06168  loss_rpn_loc: 0.06575    time: 2.9516  last_time: 2.8542  data_time: 0.0010  last_data_time: 0.0009   lr: 0.0035964  \n",
            "\u001b[32m[05/24 11:59:24 d2.utils.events]: \u001b[0m eta: 1:32:24  iter: 199  total_loss: 1.325  loss_cls: 0.3895  loss_box_reg: 0.5314  loss_mask: 0.2802  loss_rpn_cls: 0.05966  loss_rpn_loc: 0.0835    time: 2.9473  last_time: 3.2717  data_time: 0.0010  last_data_time: 0.0009   lr: 0.003996  \n",
            "\u001b[32m[05/24 12:00:26 d2.utils.events]: \u001b[0m eta: 1:31:28  iter: 219  total_loss: 1.363  loss_cls: 0.4109  loss_box_reg: 0.5354  loss_mask: 0.3045  loss_rpn_cls: 0.08327  loss_rpn_loc: 0.07629    time: 2.9623  last_time: 2.9477  data_time: 0.0011  last_data_time: 0.0010   lr: 0.0043956  \n",
            "\u001b[32m[05/24 12:01:26 d2.utils.events]: \u001b[0m eta: 1:30:37  iter: 239  total_loss: 1.292  loss_cls: 0.3836  loss_box_reg: 0.5001  loss_mask: 0.3062  loss_rpn_cls: 0.05181  loss_rpn_loc: 0.06173    time: 2.9650  last_time: 3.0426  data_time: 0.0011  last_data_time: 0.0016   lr: 0.0047952  \n",
            "\u001b[32m[05/24 12:02:27 d2.utils.events]: \u001b[0m eta: 1:30:03  iter: 259  total_loss: 1.354  loss_cls: 0.3919  loss_box_reg: 0.5041  loss_mask: 0.2826  loss_rpn_cls: 0.06057  loss_rpn_loc: 0.07493    time: 2.9717  last_time: 3.3097  data_time: 0.0010  last_data_time: 0.0008   lr: 0.0051948  \n",
            "\u001b[32m[05/24 12:03:27 d2.utils.events]: \u001b[0m eta: 1:28:47  iter: 279  total_loss: 1.376  loss_cls: 0.4101  loss_box_reg: 0.5472  loss_mask: 0.3024  loss_rpn_cls: 0.05121  loss_rpn_loc: 0.0819    time: 2.9742  last_time: 3.2521  data_time: 0.0010  last_data_time: 0.0018   lr: 0.0055944  \n",
            "\u001b[32m[05/24 12:04:25 d2.utils.events]: \u001b[0m eta: 1:27:25  iter: 299  total_loss: 1.315  loss_cls: 0.3646  loss_box_reg: 0.5128  loss_mask: 0.2769  loss_rpn_cls: 0.04697  loss_rpn_loc: 0.05075    time: 2.9683  last_time: 2.8670  data_time: 0.0011  last_data_time: 0.0015   lr: 0.005994  \n",
            "\u001b[32m[05/24 12:05:24 d2.utils.events]: \u001b[0m eta: 1:26:20  iter: 319  total_loss: 1.234  loss_cls: 0.3565  loss_box_reg: 0.4706  loss_mask: 0.2672  loss_rpn_cls: 0.05823  loss_rpn_loc: 0.0788    time: 2.9679  last_time: 3.2785  data_time: 0.0009  last_data_time: 0.0008   lr: 0.0063936  \n",
            "\u001b[32m[05/24 12:06:26 d2.utils.events]: \u001b[0m eta: 1:25:33  iter: 339  total_loss: 1.255  loss_cls: 0.3696  loss_box_reg: 0.4828  loss_mask: 0.2758  loss_rpn_cls: 0.08284  loss_rpn_loc: 0.07926    time: 2.9730  last_time: 3.1250  data_time: 0.0009  last_data_time: 0.0008   lr: 0.0067932  \n",
            "\u001b[32m[05/24 12:07:24 d2.utils.events]: \u001b[0m eta: 1:24:19  iter: 359  total_loss: 1.323  loss_cls: 0.3448  loss_box_reg: 0.5206  loss_mask: 0.3058  loss_rpn_cls: 0.05896  loss_rpn_loc: 0.07699    time: 2.9714  last_time: 3.1824  data_time: 0.0010  last_data_time: 0.0009   lr: 0.0071928  \n",
            "\u001b[32m[05/24 12:08:21 d2.utils.events]: \u001b[0m eta: 1:23:05  iter: 379  total_loss: 1.388  loss_cls: 0.3767  loss_box_reg: 0.5974  loss_mask: 0.2803  loss_rpn_cls: 0.07162  loss_rpn_loc: 0.08007    time: 2.9629  last_time: 3.1680  data_time: 0.0010  last_data_time: 0.0015   lr: 0.0075924  \n",
            "\u001b[32m[05/24 12:09:20 d2.utils.events]: \u001b[0m eta: 1:22:04  iter: 399  total_loss: 1.289  loss_cls: 0.3699  loss_box_reg: 0.5368  loss_mask: 0.2832  loss_rpn_cls: 0.05337  loss_rpn_loc: 0.05795    time: 2.9626  last_time: 3.1404  data_time: 0.0010  last_data_time: 0.0012   lr: 0.007992  \n",
            "\u001b[32m[05/24 12:10:18 d2.utils.events]: \u001b[0m eta: 1:21:04  iter: 419  total_loss: 1.36  loss_cls: 0.3726  loss_box_reg: 0.5314  loss_mask: 0.2816  loss_rpn_cls: 0.05609  loss_rpn_loc: 0.06544    time: 2.9604  last_time: 3.1084  data_time: 0.0010  last_data_time: 0.0009   lr: 0.0083916  \n",
            "\u001b[32m[05/24 12:11:17 d2.utils.events]: \u001b[0m eta: 1:20:03  iter: 439  total_loss: 1.299  loss_cls: 0.3508  loss_box_reg: 0.5359  loss_mask: 0.2767  loss_rpn_cls: 0.06542  loss_rpn_loc: 0.06984    time: 2.9597  last_time: 3.2339  data_time: 0.0011  last_data_time: 0.0010   lr: 0.0087912  \n",
            "\u001b[32m[05/24 12:12:16 d2.utils.events]: \u001b[0m eta: 1:19:01  iter: 459  total_loss: 1.341  loss_cls: 0.3902  loss_box_reg: 0.5737  loss_mask: 0.281  loss_rpn_cls: 0.05818  loss_rpn_loc: 0.08376    time: 2.9601  last_time: 3.1298  data_time: 0.0011  last_data_time: 0.0008   lr: 0.0091908  \n",
            "\u001b[32m[05/24 12:13:16 d2.utils.events]: \u001b[0m eta: 1:18:00  iter: 479  total_loss: 1.287  loss_cls: 0.3635  loss_box_reg: 0.5307  loss_mask: 0.2834  loss_rpn_cls: 0.05533  loss_rpn_loc: 0.06826    time: 2.9610  last_time: 3.1880  data_time: 0.0010  last_data_time: 0.0009   lr: 0.0095904  \n",
            "\u001b[32m[05/24 12:14:17 d2.utils.events]: \u001b[0m eta: 1:17:02  iter: 499  total_loss: 1.274  loss_cls: 0.3769  loss_box_reg: 0.4858  loss_mask: 0.2825  loss_rpn_cls: 0.05462  loss_rpn_loc: 0.05666    time: 2.9638  last_time: 3.1164  data_time: 0.0011  last_data_time: 0.0008   lr: 0.00999  \n",
            "\u001b[32m[05/24 12:15:16 d2.utils.events]: \u001b[0m eta: 1:16:01  iter: 519  total_loss: 1.174  loss_cls: 0.3417  loss_box_reg: 0.5054  loss_mask: 0.2888  loss_rpn_cls: 0.04499  loss_rpn_loc: 0.04296    time: 2.9631  last_time: 3.1603  data_time: 0.0009  last_data_time: 0.0008   lr: 0.01039  \n",
            "\u001b[32m[05/24 12:16:14 d2.utils.events]: \u001b[0m eta: 1:14:56  iter: 539  total_loss: 1.416  loss_cls: 0.3972  loss_box_reg: 0.5393  loss_mask: 0.2736  loss_rpn_cls: 0.06665  loss_rpn_loc: 0.08375    time: 2.9612  last_time: 2.8663  data_time: 0.0010  last_data_time: 0.0009   lr: 0.010789  \n",
            "\u001b[32m[05/24 12:17:11 d2.utils.events]: \u001b[0m eta: 1:13:53  iter: 559  total_loss: 1.298  loss_cls: 0.3439  loss_box_reg: 0.5089  loss_mask: 0.2833  loss_rpn_cls: 0.05202  loss_rpn_loc: 0.06722    time: 2.9583  last_time: 2.7317  data_time: 0.0010  last_data_time: 0.0012   lr: 0.011189  \n",
            "\u001b[32m[05/24 12:18:11 d2.utils.events]: \u001b[0m eta: 1:12:52  iter: 579  total_loss: 1.51  loss_cls: 0.4111  loss_box_reg: 0.575  loss_mask: 0.3009  loss_rpn_cls: 0.06863  loss_rpn_loc: 0.126    time: 2.9586  last_time: 3.2623  data_time: 0.0010  last_data_time: 0.0010   lr: 0.011588  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 12:19:08 d2.utils.events]: \u001b[0m eta: 1:11:51  iter: 599  total_loss: 1.29  loss_cls: 0.3898  loss_box_reg: 0.4546  loss_mask: 0.2693  loss_rpn_cls: 0.0507  loss_rpn_loc: 0.0782    time: 2.9559  last_time: 2.6131  data_time: 0.0010  last_data_time: 0.0008   lr: 0.011988  \n",
            "\u001b[32m[05/24 12:20:05 d2.utils.events]: \u001b[0m eta: 1:10:48  iter: 619  total_loss: 1.32  loss_cls: 0.3535  loss_box_reg: 0.5626  loss_mask: 0.2834  loss_rpn_cls: 0.05055  loss_rpn_loc: 0.07813    time: 2.9512  last_time: 3.2607  data_time: 0.0010  last_data_time: 0.0013   lr: 0.012388  \n",
            "\u001b[32m[05/24 12:21:02 d2.utils.events]: \u001b[0m eta: 1:09:44  iter: 639  total_loss: 1.428  loss_cls: 0.4153  loss_box_reg: 0.5593  loss_mask: 0.3074  loss_rpn_cls: 0.06846  loss_rpn_loc: 0.07912    time: 2.9487  last_time: 2.6163  data_time: 0.0009  last_data_time: 0.0008   lr: 0.012787  \n",
            "\u001b[32m[05/24 12:22:01 d2.utils.events]: \u001b[0m eta: 1:08:41  iter: 659  total_loss: 1.324  loss_cls: 0.3669  loss_box_reg: 0.5647  loss_mask: 0.2699  loss_rpn_cls: 0.06594  loss_rpn_loc: 0.07313    time: 2.9485  last_time: 2.8257  data_time: 0.0011  last_data_time: 0.0008   lr: 0.013187  \n",
            "\u001b[32m[05/24 12:22:58 d2.utils.events]: \u001b[0m eta: 1:07:32  iter: 679  total_loss: 1.288  loss_cls: 0.3309  loss_box_reg: 0.5115  loss_mask: 0.2777  loss_rpn_cls: 0.05047  loss_rpn_loc: 0.06094    time: 2.9461  last_time: 2.7112  data_time: 0.0010  last_data_time: 0.0014   lr: 0.013586  \n",
            "\u001b[32m[05/24 12:23:58 d2.utils.events]: \u001b[0m eta: 1:06:30  iter: 699  total_loss: 1.326  loss_cls: 0.3711  loss_box_reg: 0.5242  loss_mask: 0.2708  loss_rpn_cls: 0.05659  loss_rpn_loc: 0.07763    time: 2.9467  last_time: 2.4479  data_time: 0.0010  last_data_time: 0.0008   lr: 0.013986  \n",
            "\u001b[32m[05/24 12:24:56 d2.utils.events]: \u001b[0m eta: 1:05:27  iter: 719  total_loss: 1.438  loss_cls: 0.4017  loss_box_reg: 0.5699  loss_mask: 0.3105  loss_rpn_cls: 0.07202  loss_rpn_loc: 0.0896    time: 2.9457  last_time: 3.1122  data_time: 0.0010  last_data_time: 0.0011   lr: 0.014386  \n",
            "\u001b[32m[05/24 12:25:53 d2.utils.events]: \u001b[0m eta: 1:04:19  iter: 739  total_loss: 1.305  loss_cls: 0.3664  loss_box_reg: 0.5307  loss_mask: 0.2717  loss_rpn_cls: 0.05773  loss_rpn_loc: 0.07163    time: 2.9439  last_time: 3.2385  data_time: 0.0010  last_data_time: 0.0008   lr: 0.014785  \n",
            "\u001b[32m[05/24 12:26:53 d2.utils.events]: \u001b[0m eta: 1:03:23  iter: 759  total_loss: 1.342  loss_cls: 0.3658  loss_box_reg: 0.5283  loss_mask: 0.2896  loss_rpn_cls: 0.04851  loss_rpn_loc: 0.0568    time: 2.9443  last_time: 2.9177  data_time: 0.0009  last_data_time: 0.0009   lr: 0.015185  \n",
            "\u001b[32m[05/24 12:27:53 d2.utils.events]: \u001b[0m eta: 1:02:25  iter: 779  total_loss: 1.307  loss_cls: 0.3552  loss_box_reg: 0.5032  loss_mask: 0.2659  loss_rpn_cls: 0.05862  loss_rpn_loc: 0.0722    time: 2.9461  last_time: 2.8254  data_time: 0.0010  last_data_time: 0.0012   lr: 0.015584  \n",
            "\u001b[32m[05/24 12:28:54 d2.utils.events]: \u001b[0m eta: 1:01:30  iter: 799  total_loss: 1.447  loss_cls: 0.3828  loss_box_reg: 0.5417  loss_mask: 0.2819  loss_rpn_cls: 0.06735  loss_rpn_loc: 0.0949    time: 2.9483  last_time: 3.1203  data_time: 0.0010  last_data_time: 0.0012   lr: 0.015984  \n",
            "\u001b[32m[05/24 12:29:52 d2.utils.events]: \u001b[0m eta: 1:00:31  iter: 819  total_loss: 1.479  loss_cls: 0.3915  loss_box_reg: 0.5857  loss_mask: 0.2989  loss_rpn_cls: 0.05833  loss_rpn_loc: 0.08537    time: 2.9480  last_time: 3.1023  data_time: 0.0011  last_data_time: 0.0010   lr: 0.016384  \n",
            "\u001b[32m[05/24 12:30:48 d2.utils.events]: \u001b[0m eta: 0:59:23  iter: 839  total_loss: 1.355  loss_cls: 0.3339  loss_box_reg: 0.5908  loss_mask: 0.2841  loss_rpn_cls: 0.06059  loss_rpn_loc: 0.05465    time: 2.9443  last_time: 2.9274  data_time: 0.0009  last_data_time: 0.0008   lr: 0.016783  \n",
            "\u001b[32m[05/24 12:31:46 d2.utils.events]: \u001b[0m eta: 0:58:19  iter: 859  total_loss: 1.341  loss_cls: 0.3533  loss_box_reg: 0.5494  loss_mask: 0.2699  loss_rpn_cls: 0.05926  loss_rpn_loc: 0.07041    time: 2.9428  last_time: 2.5156  data_time: 0.0009  last_data_time: 0.0009   lr: 0.017183  \n",
            "\u001b[32m[05/24 12:32:47 d2.utils.events]: \u001b[0m eta: 0:57:24  iter: 879  total_loss: 1.455  loss_cls: 0.43  loss_box_reg: 0.5733  loss_mask: 0.2825  loss_rpn_cls: 0.05995  loss_rpn_loc: 0.09338    time: 2.9453  last_time: 3.2181  data_time: 0.0010  last_data_time: 0.0010   lr: 0.017582  \n",
            "\u001b[32m[05/24 12:33:45 d2.utils.events]: \u001b[0m eta: 0:56:20  iter: 899  total_loss: 1.444  loss_cls: 0.3789  loss_box_reg: 0.5951  loss_mask: 0.2803  loss_rpn_cls: 0.06955  loss_rpn_loc: 0.06323    time: 2.9449  last_time: 2.6247  data_time: 0.0010  last_data_time: 0.0010   lr: 0.017982  \n",
            "\u001b[32m[05/24 12:34:43 d2.utils.events]: \u001b[0m eta: 0:55:15  iter: 919  total_loss: 1.413  loss_cls: 0.4043  loss_box_reg: 0.5338  loss_mask: 0.291  loss_rpn_cls: 0.07253  loss_rpn_loc: 0.09623    time: 2.9438  last_time: 3.1792  data_time: 0.0009  last_data_time: 0.0010   lr: 0.018382  \n",
            "\u001b[32m[05/24 12:35:41 d2.utils.events]: \u001b[0m eta: 0:54:14  iter: 939  total_loss: 1.299  loss_cls: 0.3611  loss_box_reg: 0.5031  loss_mask: 0.2599  loss_rpn_cls: 0.05638  loss_rpn_loc: 0.06675    time: 2.9427  last_time: 3.0799  data_time: 0.0010  last_data_time: 0.0009   lr: 0.018781  \n",
            "\u001b[32m[05/24 12:36:40 d2.utils.events]: \u001b[0m eta: 0:53:12  iter: 959  total_loss: 1.171  loss_cls: 0.3256  loss_box_reg: 0.5331  loss_mask: 0.2503  loss_rpn_cls: 0.05183  loss_rpn_loc: 0.0574    time: 2.9428  last_time: 3.1748  data_time: 0.0009  last_data_time: 0.0010   lr: 0.019181  \n",
            "\u001b[32m[05/24 12:37:36 d2.utils.events]: \u001b[0m eta: 0:52:08  iter: 979  total_loss: 1.396  loss_cls: 0.3908  loss_box_reg: 0.547  loss_mask: 0.281  loss_rpn_cls: 0.08005  loss_rpn_loc: 0.08632    time: 2.9402  last_time: 2.8624  data_time: 0.0010  last_data_time: 0.0008   lr: 0.01958  \n",
            "\u001b[32m[05/24 12:38:35 d2.utils.events]: \u001b[0m eta: 0:51:07  iter: 999  total_loss: 1.421  loss_cls: 0.3809  loss_box_reg: 0.5734  loss_mask: 0.279  loss_rpn_cls: 0.07259  loss_rpn_loc: 0.08531    time: 2.9396  last_time: 3.2356  data_time: 0.0009  last_data_time: 0.0009   lr: 0.01998  \n",
            "\u001b[32m[05/24 12:39:33 d2.utils.events]: \u001b[0m eta: 0:50:04  iter: 1019  total_loss: 1.468  loss_cls: 0.4035  loss_box_reg: 0.5831  loss_mask: 0.3025  loss_rpn_cls: 0.08093  loss_rpn_loc: 0.09387    time: 2.9393  last_time: 3.0941  data_time: 0.0010  last_data_time: 0.0014   lr: 0.02  \n",
            "\u001b[32m[05/24 12:40:31 d2.utils.events]: \u001b[0m eta: 0:49:02  iter: 1039  total_loss: 1.458  loss_cls: 0.3998  loss_box_reg: 0.6041  loss_mask: 0.3046  loss_rpn_cls: 0.07748  loss_rpn_loc: 0.09068    time: 2.9389  last_time: 3.1410  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:41:29 d2.utils.events]: \u001b[0m eta: 0:48:05  iter: 1059  total_loss: 1.339  loss_cls: 0.3379  loss_box_reg: 0.5431  loss_mask: 0.2705  loss_rpn_cls: 0.05832  loss_rpn_loc: 0.07507    time: 2.9382  last_time: 2.5815  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:42:29 d2.utils.events]: \u001b[0m eta: 0:47:08  iter: 1079  total_loss: 1.38  loss_cls: 0.3645  loss_box_reg: 0.574  loss_mask: 0.2676  loss_rpn_cls: 0.0584  loss_rpn_loc: 0.07464    time: 2.9392  last_time: 3.1903  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:43:28 d2.utils.events]: \u001b[0m eta: 0:46:05  iter: 1099  total_loss: 1.444  loss_cls: 0.3952  loss_box_reg: 0.583  loss_mask: 0.2868  loss_rpn_cls: 0.07704  loss_rpn_loc: 0.09154    time: 2.9393  last_time: 2.8841  data_time: 0.0010  last_data_time: 0.0011   lr: 0.02  \n",
            "\u001b[32m[05/24 12:44:27 d2.utils.events]: \u001b[0m eta: 0:45:06  iter: 1119  total_loss: 1.403  loss_cls: 0.3917  loss_box_reg: 0.5564  loss_mask: 0.2775  loss_rpn_cls: 0.06643  loss_rpn_loc: 0.09474    time: 2.9395  last_time: 2.3827  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:45:26 d2.utils.events]: \u001b[0m eta: 0:44:02  iter: 1139  total_loss: 1.412  loss_cls: 0.3683  loss_box_reg: 0.5723  loss_mask: 0.3061  loss_rpn_cls: 0.06077  loss_rpn_loc: 0.08356    time: 2.9394  last_time: 2.8586  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 12:46:25 d2.utils.events]: \u001b[0m eta: 0:42:59  iter: 1159  total_loss: 1.347  loss_cls: 0.3927  loss_box_reg: 0.5593  loss_mask: 0.2778  loss_rpn_cls: 0.05417  loss_rpn_loc: 0.05015    time: 2.9396  last_time: 2.5651  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 12:47:21 d2.utils.events]: \u001b[0m eta: 0:41:44  iter: 1179  total_loss: 1.275  loss_cls: 0.3127  loss_box_reg: 0.5213  loss_mask: 0.266  loss_rpn_cls: 0.07022  loss_rpn_loc: 0.07105    time: 2.9377  last_time: 3.0518  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 12:48:20 d2.utils.events]: \u001b[0m eta: 0:40:44  iter: 1199  total_loss: 1.399  loss_cls: 0.3844  loss_box_reg: 0.5638  loss_mask: 0.2893  loss_rpn_cls: 0.06189  loss_rpn_loc: 0.09168    time: 2.9378  last_time: 2.8228  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 12:49:19 d2.utils.events]: \u001b[0m eta: 0:39:41  iter: 1219  total_loss: 1.314  loss_cls: 0.3348  loss_box_reg: 0.5402  loss_mask: 0.2691  loss_rpn_cls: 0.06441  loss_rpn_loc: 0.08546    time: 2.9381  last_time: 3.1292  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 12:50:18 d2.utils.events]: \u001b[0m eta: 0:38:42  iter: 1239  total_loss: 1.274  loss_cls: 0.3546  loss_box_reg: 0.5375  loss_mask: 0.2631  loss_rpn_cls: 0.04411  loss_rpn_loc: 0.07814    time: 2.9379  last_time: 2.6223  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:51:15 d2.utils.events]: \u001b[0m eta: 0:37:30  iter: 1259  total_loss: 1.353  loss_cls: 0.364  loss_box_reg: 0.5419  loss_mask: 0.2697  loss_rpn_cls: 0.0788  loss_rpn_loc: 0.08915    time: 2.9367  last_time: 3.1358  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:52:15 d2.utils.events]: \u001b[0m eta: 0:36:34  iter: 1279  total_loss: 1.458  loss_cls: 0.3782  loss_box_reg: 0.5851  loss_mask: 0.289  loss_rpn_cls: 0.06502  loss_rpn_loc: 0.07523    time: 2.9377  last_time: 3.1562  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 12:53:14 d2.utils.events]: \u001b[0m eta: 0:35:38  iter: 1299  total_loss: 1.337  loss_cls: 0.3524  loss_box_reg: 0.5448  loss_mask: 0.2499  loss_rpn_cls: 0.06738  loss_rpn_loc: 0.07964    time: 2.9377  last_time: 2.8188  data_time: 0.0010  last_data_time: 0.0014   lr: 0.02  \n",
            "\u001b[32m[05/24 12:54:13 d2.utils.events]: \u001b[0m eta: 0:34:38  iter: 1319  total_loss: 1.38  loss_cls: 0.3712  loss_box_reg: 0.5605  loss_mask: 0.2777  loss_rpn_cls: 0.06134  loss_rpn_loc: 0.09353    time: 2.9381  last_time: 3.2195  data_time: 0.0011  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 12:55:11 d2.utils.events]: \u001b[0m eta: 0:33:25  iter: 1339  total_loss: 1.292  loss_cls: 0.3465  loss_box_reg: 0.5519  loss_mask: 0.2874  loss_rpn_cls: 0.04558  loss_rpn_loc: 0.07304    time: 2.9374  last_time: 3.2033  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:56:11 d2.utils.events]: \u001b[0m eta: 0:32:26  iter: 1359  total_loss: 1.333  loss_cls: 0.3755  loss_box_reg: 0.5307  loss_mask: 0.2694  loss_rpn_cls: 0.05317  loss_rpn_loc: 0.09167    time: 2.9385  last_time: 2.8407  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 12:57:10 d2.utils.events]: \u001b[0m eta: 0:31:34  iter: 1379  total_loss: 1.203  loss_cls: 0.3163  loss_box_reg: 0.5247  loss_mask: 0.2511  loss_rpn_cls: 0.05085  loss_rpn_loc: 0.0656    time: 2.9381  last_time: 2.3563  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 12:58:10 d2.utils.events]: \u001b[0m eta: 0:30:37  iter: 1399  total_loss: 1.385  loss_cls: 0.3957  loss_box_reg: 0.531  loss_mask: 0.2643  loss_rpn_cls: 0.05254  loss_rpn_loc: 0.08291    time: 2.9394  last_time: 2.3950  data_time: 0.0009  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 12:59:08 d2.utils.events]: \u001b[0m eta: 0:29:28  iter: 1419  total_loss: 1.333  loss_cls: 0.3398  loss_box_reg: 0.5397  loss_mask: 0.27  loss_rpn_cls: 0.05576  loss_rpn_loc: 0.06144    time: 2.9390  last_time: 2.7972  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:00:07 d2.utils.events]: \u001b[0m eta: 0:28:24  iter: 1439  total_loss: 1.351  loss_cls: 0.3314  loss_box_reg: 0.5694  loss_mask: 0.2815  loss_rpn_cls: 0.06349  loss_rpn_loc: 0.07948    time: 2.9391  last_time: 3.1294  data_time: 0.0010  last_data_time: 0.0014   lr: 0.02  \n",
            "\u001b[32m[05/24 13:01:06 d2.utils.events]: \u001b[0m eta: 0:27:23  iter: 1459  total_loss: 1.218  loss_cls: 0.3503  loss_box_reg: 0.5039  loss_mask: 0.255  loss_rpn_cls: 0.05158  loss_rpn_loc: 0.07823    time: 2.9393  last_time: 3.1089  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:02:05 d2.utils.events]: \u001b[0m eta: 0:26:22  iter: 1479  total_loss: 1.258  loss_cls: 0.3424  loss_box_reg: 0.5302  loss_mask: 0.2585  loss_rpn_cls: 0.03686  loss_rpn_loc: 0.0601    time: 2.9394  last_time: 3.1425  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:03:06 d2.utils.events]: \u001b[0m eta: 0:25:23  iter: 1499  total_loss: 1.225  loss_cls: 0.3193  loss_box_reg: 0.5  loss_mask: 0.2443  loss_rpn_cls: 0.05277  loss_rpn_loc: 0.08055    time: 2.9408  last_time: 2.5997  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:04:09 d2.utils.events]: \u001b[0m eta: 0:24:29  iter: 1519  total_loss: 1.397  loss_cls: 0.3817  loss_box_reg: 0.517  loss_mask: 0.2687  loss_rpn_cls: 0.05082  loss_rpn_loc: 0.09136    time: 2.9431  last_time: 3.1043  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:05:09 d2.utils.events]: \u001b[0m eta: 0:23:32  iter: 1539  total_loss: 1.339  loss_cls: 0.368  loss_box_reg: 0.5348  loss_mask: 0.2734  loss_rpn_cls: 0.04867  loss_rpn_loc: 0.07957    time: 2.9444  last_time: 3.2562  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:06:10 d2.utils.events]: \u001b[0m eta: 0:22:32  iter: 1559  total_loss: 1.079  loss_cls: 0.2788  loss_box_reg: 0.4667  loss_mask: 0.2226  loss_rpn_cls: 0.03871  loss_rpn_loc: 0.06395    time: 2.9454  last_time: 3.1881  data_time: 0.0009  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:07:09 d2.utils.events]: \u001b[0m eta: 0:21:31  iter: 1579  total_loss: 1.307  loss_cls: 0.3743  loss_box_reg: 0.5275  loss_mask: 0.257  loss_rpn_cls: 0.05105  loss_rpn_loc: 0.1113    time: 2.9456  last_time: 2.6579  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:08:08 d2.utils.events]: \u001b[0m eta: 0:20:29  iter: 1599  total_loss: 1.274  loss_cls: 0.3348  loss_box_reg: 0.5175  loss_mask: 0.2563  loss_rpn_cls: 0.04815  loss_rpn_loc: 0.07281    time: 2.9453  last_time: 3.1885  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:09:07 d2.utils.events]: \u001b[0m eta: 0:19:28  iter: 1619  total_loss: 1.167  loss_cls: 0.3264  loss_box_reg: 0.5038  loss_mask: 0.2541  loss_rpn_cls: 0.04725  loss_rpn_loc: 0.0916    time: 2.9456  last_time: 3.2517  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:10:07 d2.utils.events]: \u001b[0m eta: 0:18:27  iter: 1639  total_loss: 1.384  loss_cls: 0.3703  loss_box_reg: 0.5424  loss_mask: 0.2811  loss_rpn_cls: 0.05688  loss_rpn_loc: 0.08652    time: 2.9464  last_time: 2.6003  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:11:08 d2.utils.events]: \u001b[0m eta: 0:17:27  iter: 1659  total_loss: 1.209  loss_cls: 0.302  loss_box_reg: 0.4967  loss_mask: 0.2395  loss_rpn_cls: 0.05423  loss_rpn_loc: 0.0733    time: 2.9473  last_time: 2.6210  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:12:07 d2.utils.events]: \u001b[0m eta: 0:16:25  iter: 1679  total_loss: 1.274  loss_cls: 0.3497  loss_box_reg: 0.5018  loss_mask: 0.2795  loss_rpn_cls: 0.05317  loss_rpn_loc: 0.117    time: 2.9474  last_time: 2.8559  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:13:06 d2.utils.events]: \u001b[0m eta: 0:15:25  iter: 1699  total_loss: 1.147  loss_cls: 0.2903  loss_box_reg: 0.5064  loss_mask: 0.2557  loss_rpn_cls: 0.03613  loss_rpn_loc: 0.0508    time: 2.9477  last_time: 3.1698  data_time: 0.0009  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:14:07 d2.utils.events]: \u001b[0m eta: 0:14:24  iter: 1719  total_loss: 1.146  loss_cls: 0.2986  loss_box_reg: 0.4934  loss_mask: 0.2217  loss_rpn_cls: 0.03937  loss_rpn_loc: 0.06295    time: 2.9487  last_time: 2.8784  data_time: 0.0010  last_data_time: 0.0011   lr: 0.02  \n",
            "\u001b[32m[05/24 13:15:07 d2.utils.events]: \u001b[0m eta: 0:13:24  iter: 1739  total_loss: 1.125  loss_cls: 0.2917  loss_box_reg: 0.4872  loss_mask: 0.2339  loss_rpn_cls: 0.05099  loss_rpn_loc: 0.06916    time: 2.9491  last_time: 3.1806  data_time: 0.0010  last_data_time: 0.0013   lr: 0.02  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[05/24 13:16:05 d2.utils.events]: \u001b[0m eta: 0:12:22  iter: 1759  total_loss: 1.209  loss_cls: 0.3419  loss_box_reg: 0.4833  loss_mask: 0.2537  loss_rpn_cls: 0.04366  loss_rpn_loc: 0.06368    time: 2.9489  last_time: 2.5940  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:17:03 d2.utils.events]: \u001b[0m eta: 0:11:19  iter: 1779  total_loss: 1.196  loss_cls: 0.3175  loss_box_reg: 0.5175  loss_mask: 0.2802  loss_rpn_cls: 0.04505  loss_rpn_loc: 0.07562    time: 2.9485  last_time: 3.1677  data_time: 0.0009  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:18:02 d2.utils.events]: \u001b[0m eta: 0:10:16  iter: 1799  total_loss: 1.258  loss_cls: 0.3377  loss_box_reg: 0.5193  loss_mask: 0.2547  loss_rpn_cls: 0.0422  loss_rpn_loc: 0.06467    time: 2.9481  last_time: 3.1608  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:19:01 d2.utils.events]: \u001b[0m eta: 0:09:14  iter: 1819  total_loss: 1.326  loss_cls: 0.3345  loss_box_reg: 0.5169  loss_mask: 0.2757  loss_rpn_cls: 0.08374  loss_rpn_loc: 0.09806    time: 2.9485  last_time: 2.8818  data_time: 0.0010  last_data_time: 0.0009   lr: 0.02  \n",
            "\u001b[32m[05/24 13:20:02 d2.utils.events]: \u001b[0m eta: 0:08:14  iter: 1839  total_loss: 1.149  loss_cls: 0.3115  loss_box_reg: 0.4965  loss_mask: 0.2344  loss_rpn_cls: 0.05308  loss_rpn_loc: 0.07005    time: 2.9495  last_time: 3.2210  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:21:00 d2.utils.events]: \u001b[0m eta: 0:07:12  iter: 1859  total_loss: 1.211  loss_cls: 0.3214  loss_box_reg: 0.4664  loss_mask: 0.2462  loss_rpn_cls: 0.0539  loss_rpn_loc: 0.08194    time: 2.9490  last_time: 3.1384  data_time: 0.0010  last_data_time: 0.0008   lr: 0.02  \n",
            "\u001b[32m[05/24 13:21:59 d2.utils.events]: \u001b[0m eta: 0:06:10  iter: 1879  total_loss: 1.303  loss_cls: 0.3385  loss_box_reg: 0.5026  loss_mask: 0.2604  loss_rpn_cls: 0.04208  loss_rpn_loc: 0.08503    time: 2.9491  last_time: 3.1682  data_time: 0.0010  last_data_time: 0.0013   lr: 0.02  \n",
            "\u001b[32m[05/24 13:22:59 d2.utils.events]: \u001b[0m eta: 0:05:09  iter: 1899  total_loss: 1.27  loss_cls: 0.3378  loss_box_reg: 0.5065  loss_mask: 0.2468  loss_rpn_cls: 0.03307  loss_rpn_loc: 0.07715    time: 2.9495  last_time: 2.6305  data_time: 0.0010  last_data_time: 0.0015   lr: 0.02  \n",
            "\u001b[32m[05/24 13:23:55 d2.utils.events]: \u001b[0m eta: 0:04:07  iter: 1919  total_loss: 1.076  loss_cls: 0.3101  loss_box_reg: 0.4693  loss_mask: 0.2381  loss_rpn_cls: 0.02884  loss_rpn_loc: 0.05516    time: 2.9481  last_time: 2.8300  data_time: 0.0010  last_data_time: 0.0013   lr: 0.02  \n",
            "\u001b[32m[05/24 13:24:56 d2.utils.events]: \u001b[0m eta: 0:03:05  iter: 1939  total_loss: 1.138  loss_cls: 0.2942  loss_box_reg: 0.464  loss_mask: 0.2403  loss_rpn_cls: 0.0337  loss_rpn_loc: 0.0867    time: 2.9489  last_time: 2.8502  data_time: 0.0010  last_data_time: 0.0012   lr: 0.02  \n",
            "\u001b[32m[05/24 13:25:56 d2.utils.events]: \u001b[0m eta: 0:02:03  iter: 1959  total_loss: 1.171  loss_cls: 0.3324  loss_box_reg: 0.4822  loss_mask: 0.2357  loss_rpn_cls: 0.05066  loss_rpn_loc: 0.09081    time: 2.9494  last_time: 3.2464  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:26:56 d2.utils.events]: \u001b[0m eta: 0:01:01  iter: 1979  total_loss: 1.061  loss_cls: 0.2698  loss_box_reg: 0.4726  loss_mask: 0.2235  loss_rpn_cls: 0.0332  loss_rpn_loc: 0.06691    time: 2.9501  last_time: 2.8315  data_time: 0.0010  last_data_time: 0.0010   lr: 0.02  \n",
            "\u001b[32m[05/24 13:27:57 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 1999  total_loss: 1.068  loss_cls: 0.2715  loss_box_reg: 0.4604  loss_mask: 0.2269  loss_rpn_cls: 0.03771  loss_rpn_loc: 0.06722    time: 2.9506  last_time: 3.1406  data_time: 0.0009  last_data_time: 0.0015   lr: 0.02  \n",
            "\u001b[32m[05/24 13:27:57 d2.engine.hooks]: \u001b[0mOverall training speed: 1998 iterations in 1:38:15 (2.9506 s / it)\n",
            "\u001b[32m[05/24 13:27:57 d2.engine.hooks]: \u001b[0mTotal training time: 1:38:15 (0:00:00 on hooks)\n"
          ]
        }
      ],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "#specify the path to your image and json annotation file (COCO)\n",
        "register_coco_instances(\"satellite_dataset\", {}, \"./labeled_sat_images/result.json\", \"./labeled_sat_images/images\")\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "#choose a model from model zoo\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"satellite_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  #para testes\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 #número de workers para carregar os dados\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  #initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2 #Batch Size\n",
        "cfg.SOLVER.BASE_LR = 0.02  #Learning Rate\n",
        "cfg.SOLVER.MAX_ITER = 2000  #Number of iterations\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  #my classes: rooftop and solar\n",
        "cfg.DATALOADER.NUM_WORKERS = 4  #Number of data loading workers (CPU cores, talvez GPUs)\n",
        "\n",
        "#set device to CPU, n funciona com MPS, so CUDA\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "#create a Trainer and starts training!\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca9bc0f",
      "metadata": {
        "id": "2ca9bc0f"
      },
      "source": [
        "Depois desse primeiro modelo funcional começamos a pesquisar formas de melhora-lo. Uma das melhores formas de melhorar o modelo é fornecendo-lhe mais dados. Devido a o trabalho laborioso que é fazer o processo de _labeling_ dessas imagens e o pouco tempo que o projeto nos da começamos a procurar por formas alternativas de melhorar nosso modelo sem aumentar seu _dataset_.\n",
        "\n",
        "Uma dessas formas é o ajuste dos hiperparametros, tentamos aumentar o numero de iterações, _batch size_ e _learning rate_. Mas uma das formas mais interessantes que tentamos foi usando otimizadores. Em _machine learning optimizers_ são algoritmos usados para alterar os atributos do modelo de aprendizado de máquina, como pesos e taxas de aprendizado, a fim de reduzir as perdas. Os otimizadores ajudam a obter os resultados mais precisos possíveis. O otimizador que testamos é um bastante popular, o ADAM.\n",
        "\n",
        "O ADAM (Adaptive Moment Estimation) é um método de otimização que pode ser usado em vez do clássico procedimento de _stochastic gradient descent_ (SGD ou descida de gradiente estocástico) para atualizar os pesos da rede de forma iterativa com base nos dados de treinamento. Adam é um algoritmo popular porque ele supera outros otimizadores em muitos cenários e é menos sensível à inicialização dos parâmetros e a hiperparâmetros, além de ser eficiente computacionalmente.\n",
        "\n",
        "Ao utilizar o ADAM ja notamos uma grande melhora no nossa curva de perda, que tornou-se menos erratica (esses dados estão disponiveis no notebook de teste)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2bbf5ca",
      "metadata": {
        "scrolled": true,
        "id": "c2bbf5ca",
        "outputId": "934637a9-34d4-41ec-c0b2-6187847c0499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/11 17:32:57 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/11 17:32:57 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[06/11 17:32:57 d2.data.datasets.coco]: \u001b[0mLoaded 204 images in COCO format from ./labeled_sat_images/result.json\n",
            "\u001b[32m[06/11 17:32:57 d2.data.build]: \u001b[0mRemoved 1 images with no usable annotations. 203 images left.\n",
            "\u001b[32m[06/11 17:32:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[06/11 17:32:57 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[06/11 17:32:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[06/11 17:32:57 d2.data.common]: \u001b[0mSerializing 203 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[06/11 17:32:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.54 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/11 17:32:57 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[06/11 17:32:57 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (2, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/11 17:32:57 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tiberio/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using TensorFlow backend.\n",
            "/Users/tiberio/anaconda3/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/11 17:36:52 d2.utils.events]: \u001b[0m eta: 12:23:27  iter: 19  total_loss: 3.05  loss_cls: 1.153  loss_box_reg: 0.7456  loss_mask: 0.6912  loss_rpn_cls: 0.4426  loss_rpn_loc: 0.08617    time: 11.2246  last_time: 9.0106  data_time: 0.0742  last_data_time: 0.0025   lr: 1.9981e-05  \n",
            "\u001b[32m[06/11 17:40:38 d2.utils.events]: \u001b[0m eta: 12:27:57  iter: 39  total_loss: 2.593  loss_cls: 0.9283  loss_box_reg: 0.7359  loss_mask: 0.6815  loss_rpn_cls: 0.1693  loss_rpn_loc: 0.08072    time: 11.2736  last_time: 11.5533  data_time: 0.0031  last_data_time: 0.0029   lr: 3.9961e-05  \n",
            "\u001b[32m[06/11 17:44:26 d2.utils.events]: \u001b[0m eta: 12:27:15  iter: 59  total_loss: 2.349  loss_cls: 0.7024  loss_box_reg: 0.7754  loss_mask: 0.6605  loss_rpn_cls: 0.1493  loss_rpn_loc: 0.0844    time: 11.3041  last_time: 11.5726  data_time: 0.0030  last_data_time: 0.0032   lr: 5.9941e-05  \n",
            "\u001b[32m[06/11 18:04:21 d2.utils.events]: \u001b[0m eta: 12:25:45  iter: 79  total_loss: 2.215  loss_cls: 0.5997  loss_box_reg: 0.7747  loss_mask: 0.632  loss_rpn_cls: 0.1326  loss_rpn_loc: 0.07937    time: 11.3219  last_time: 11.8409  data_time: 0.0029  last_data_time: 0.0028   lr: 7.9921e-05  \n",
            "\u001b[32m[06/11 18:08:44 d2.utils.events]: \u001b[0m eta: 12:33:18  iter: 99  total_loss: 2.099  loss_cls: 0.5363  loss_box_reg: 0.7834  loss_mask: 0.6008  loss_rpn_cls: 0.1172  loss_rpn_loc: 0.07539    time: 11.6911  last_time: 14.3275  data_time: 0.0035  last_data_time: 0.0033   lr: 9.9901e-05  \n",
            "\u001b[32m[06/11 18:13:52 d2.utils.events]: \u001b[0m eta: 12:39:48  iter: 119  total_loss: 2.014  loss_cls: 0.5027  loss_box_reg: 0.792  loss_mask: 0.5577  loss_rpn_cls: 0.109  loss_rpn_loc: 0.07682    time: 12.3173  last_time: 16.5387  data_time: 0.0042  last_data_time: 0.0057   lr: 0.00011988  \n",
            "\u001b[32m[06/11 18:31:56 d2.utils.events]: \u001b[0m eta: 12:40:31  iter: 139  total_loss: 1.937  loss_cls: 0.4665  loss_box_reg: 0.7609  loss_mask: 0.5018  loss_rpn_cls: 0.1005  loss_rpn_loc: 0.07155    time: 12.8372  last_time: 11.3192  data_time: 0.0047  last_data_time: 0.0046   lr: 0.00013986  \n",
            "\u001b[32m[06/11 18:35:49 d2.utils.events]: \u001b[0m eta: 12:34:16  iter: 159  total_loss: 1.853  loss_cls: 0.4368  loss_box_reg: 0.7645  loss_mask: 0.4659  loss_rpn_cls: 0.09822  loss_rpn_loc: 0.06819    time: 12.6845  last_time: 11.1831  data_time: 0.0030  last_data_time: 0.0026   lr: 0.00015984  \n",
            "\u001b[32m[06/11 18:39:39 d2.utils.events]: \u001b[0m eta: 12:27:58  iter: 179  total_loss: 1.719  loss_cls: 0.4152  loss_box_reg: 0.7641  loss_mask: 0.4194  loss_rpn_cls: 0.07368  loss_rpn_loc: 0.06623    time: 12.5516  last_time: 10.1552  data_time: 0.0029  last_data_time: 0.0024   lr: 0.00017982  \n",
            "\u001b[32m[06/11 18:43:31 d2.utils.events]: \u001b[0m eta: 12:24:03  iter: 199  total_loss: 1.672  loss_cls: 0.3973  loss_box_reg: 0.7222  loss_mask: 0.3962  loss_rpn_cls: 0.07571  loss_rpn_loc: 0.06919    time: 12.4546  last_time: 10.9472  data_time: 0.0030  last_data_time: 0.0023   lr: 0.0001998  \n",
            "\u001b[32m[06/11 18:47:20 d2.utils.events]: \u001b[0m eta: 12:19:14  iter: 219  total_loss: 1.635  loss_cls: 0.3953  loss_box_reg: 0.6866  loss_mask: 0.3817  loss_rpn_cls: 0.08381  loss_rpn_loc: 0.07532    time: 12.3635  last_time: 12.1146  data_time: 0.0029  last_data_time: 0.0025   lr: 0.00021978  \n",
            "\u001b[32m[06/11 18:51:10 d2.utils.events]: \u001b[0m eta: 12:14:47  iter: 239  total_loss: 1.521  loss_cls: 0.374  loss_box_reg: 0.6323  loss_mask: 0.3423  loss_rpn_cls: 0.07132  loss_rpn_loc: 0.07092    time: 12.2931  last_time: 10.9257  data_time: 0.0030  last_data_time: 0.0029   lr: 0.00023976  \n",
            "\u001b[32m[06/11 18:54:59 d2.utils.events]: \u001b[0m eta: 12:10:29  iter: 259  total_loss: 1.505  loss_cls: 0.387  loss_box_reg: 0.6102  loss_mask: 0.3378  loss_rpn_cls: 0.07842  loss_rpn_loc: 0.06371    time: 12.2275  last_time: 11.6293  data_time: 0.0030  last_data_time: 0.0033   lr: 0.00025974  \n",
            "\u001b[32m[06/11 18:58:49 d2.utils.events]: \u001b[0m eta: 12:06:13  iter: 279  total_loss: 1.402  loss_cls: 0.3658  loss_box_reg: 0.5812  loss_mask: 0.3289  loss_rpn_cls: 0.06172  loss_rpn_loc: 0.06075    time: 12.1749  last_time: 11.4139  data_time: 0.0032  last_data_time: 0.0041   lr: 0.00027972  \n",
            "\u001b[32m[06/11 19:10:48 d2.utils.events]: \u001b[0m eta: 12:02:29  iter: 299  total_loss: 1.363  loss_cls: 0.3681  loss_box_reg: 0.5381  loss_mask: 0.3145  loss_rpn_cls: 0.05751  loss_rpn_loc: 0.0675    time: 12.1482  last_time: 11.6273  data_time: 0.0031  last_data_time: 0.0029   lr: 0.0002997  \n",
            "\u001b[32m[06/11 19:30:58 d2.utils.events]: \u001b[0m eta: 11:57:52  iter: 319  total_loss: 1.327  loss_cls: 0.36  loss_box_reg: 0.5279  loss_mask: 0.3054  loss_rpn_cls: 0.0608  loss_rpn_loc: 0.05777    time: 12.0958  last_time: 11.7111  data_time: 0.0029  last_data_time: 0.0029   lr: 0.00031968  \n",
            "\u001b[32m[06/11 19:35:36 d2.utils.events]: \u001b[0m eta: 11:54:44  iter: 339  total_loss: 1.295  loss_cls: 0.3501  loss_box_reg: 0.5078  loss_mask: 0.3028  loss_rpn_cls: 0.06098  loss_rpn_loc: 0.06649    time: 12.2022  last_time: 15.2204  data_time: 0.0038  last_data_time: 0.0039   lr: 0.00033966  \n",
            "\u001b[32m[06/11 19:42:06 d2.utils.events]: \u001b[0m eta: 11:52:18  iter: 359  total_loss: 1.285  loss_cls: 0.3556  loss_box_reg: 0.4998  loss_mask: 0.2985  loss_rpn_cls: 0.05659  loss_rpn_loc: 0.06837    time: 12.6097  last_time: 18.9143  data_time: 0.0059  last_data_time: 0.0052   lr: 0.00035964  \n",
            "\u001b[32m[06/11 20:07:47 d2.utils.events]: \u001b[0m eta: 11:48:26  iter: 379  total_loss: 1.308  loss_cls: 0.3495  loss_box_reg: 0.5003  loss_mask: 0.2999  loss_rpn_cls: 0.06004  loss_rpn_loc: 0.05999    time: 12.7218  last_time: 11.3824  data_time: 0.0041  last_data_time: 0.0026   lr: 0.00037962  \n",
            "\u001b[32m[06/11 20:43:29 d2.utils.events]: \u001b[0m eta: 11:43:31  iter: 399  total_loss: 1.249  loss_cls: 0.3641  loss_box_reg: 0.4905  loss_mask: 0.2939  loss_rpn_cls: 0.05312  loss_rpn_loc: 0.06245    time: 12.6578  last_time: 11.6155  data_time: 0.0031  last_data_time: 0.0031   lr: 0.0003996  \n",
            "\u001b[32m[06/11 20:47:50 d2.utils.events]: \u001b[0m eta: 11:40:58  iter: 419  total_loss: 1.193  loss_cls: 0.3523  loss_box_reg: 0.457  loss_mask: 0.2794  loss_rpn_cls: 0.05121  loss_rpn_loc: 0.06232    time: 12.6768  last_time: 14.8871  data_time: 0.0035  last_data_time: 0.0037   lr: 0.00041958  \n",
            "\u001b[32m[06/11 20:52:53 d2.utils.events]: \u001b[0m eta: 11:38:29  iter: 439  total_loss: 1.241  loss_cls: 0.3436  loss_box_reg: 0.4844  loss_mask: 0.2938  loss_rpn_cls: 0.04193  loss_rpn_loc: 0.05459    time: 12.7893  last_time: 15.9660  data_time: 0.0040  last_data_time: 0.0046   lr: 0.00043956  \n",
            "\u001b[32m[06/11 21:34:29 d2.utils.events]: \u001b[0m eta: 11:34:15  iter: 459  total_loss: 1.229  loss_cls: 0.3452  loss_box_reg: 0.4702  loss_mask: 0.2873  loss_rpn_cls: 0.04618  loss_rpn_loc: 0.06019    time: 12.7942  last_time: 11.1681  data_time: 0.0037  last_data_time: 0.0031   lr: 0.00045954  \n",
            "\u001b[32m[06/11 21:53:46 d2.utils.events]: \u001b[0m eta: 11:29:18  iter: 479  total_loss: 1.163  loss_cls: 0.3324  loss_box_reg: 0.4405  loss_mask: 0.2789  loss_rpn_cls: 0.04694  loss_rpn_loc: 0.05984    time: 12.7331  last_time: 12.6661  data_time: 0.0031  last_data_time: 0.0036   lr: 0.00047952  \n",
            "\u001b[32m[06/11 21:58:24 d2.utils.events]: \u001b[0m eta: 11:26:43  iter: 499  total_loss: 1.132  loss_cls: 0.337  loss_box_reg: 0.4221  loss_mask: 0.2682  loss_rpn_cls: 0.04422  loss_rpn_loc: 0.05531    time: 12.7813  last_time: 15.2654  data_time: 0.0037  last_data_time: 0.0033   lr: 0.0004995  \n",
            "\u001b[32m[06/11 22:03:29 d2.utils.events]: \u001b[0m eta: 11:24:35  iter: 519  total_loss: 1.148  loss_cls: 0.328  loss_box_reg: 0.4434  loss_mask: 0.2758  loss_rpn_cls: 0.04536  loss_rpn_loc: 0.05844    time: 12.8757  last_time: 16.5202  data_time: 0.0042  last_data_time: 0.0036   lr: 0.00051948  \n",
            "\u001b[32m[06/11 22:07:20 d2.utils.events]: \u001b[0m eta: 11:19:35  iter: 539  total_loss: 1.116  loss_cls: 0.3338  loss_box_reg: 0.4376  loss_mask: 0.2739  loss_rpn_cls: 0.03876  loss_rpn_loc: 0.05101    time: 12.8275  last_time: 11.7544  data_time: 0.0032  last_data_time: 0.0027   lr: 0.00053946  \n",
            "\u001b[32m[06/11 22:11:12 d2.utils.events]: \u001b[0m eta: 11:14:44  iter: 559  total_loss: 1.114  loss_cls: 0.3082  loss_box_reg: 0.4362  loss_mask: 0.2759  loss_rpn_cls: 0.04194  loss_rpn_loc: 0.05959    time: 12.7830  last_time: 11.7863  data_time: 0.0028  last_data_time: 0.0029   lr: 0.00055944  \n",
            "\u001b[32m[06/11 22:15:02 d2.utils.events]: \u001b[0m eta: 11:09:50  iter: 579  total_loss: 1.081  loss_cls: 0.3034  loss_box_reg: 0.4248  loss_mask: 0.2674  loss_rpn_cls: 0.03968  loss_rpn_loc: 0.051    time: 12.7375  last_time: 11.6020  data_time: 0.0031  last_data_time: 0.0027   lr: 0.00057942  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/11 22:18:48 d2.utils.events]: \u001b[0m eta: 11:05:32  iter: 599  total_loss: 1.083  loss_cls: 0.3211  loss_box_reg: 0.4126  loss_mask: 0.2712  loss_rpn_cls: 0.03672  loss_rpn_loc: 0.05911    time: 12.6895  last_time: 10.6665  data_time: 0.0030  last_data_time: 0.0031   lr: 0.0005994  \n",
            "\u001b[32m[06/11 22:22:35 d2.utils.events]: \u001b[0m eta: 11:00:35  iter: 619  total_loss: 1.075  loss_cls: 0.3097  loss_box_reg: 0.4094  loss_mask: 0.2633  loss_rpn_cls: 0.03623  loss_rpn_loc: 0.0555    time: 12.6461  last_time: 11.6605  data_time: 0.0030  last_data_time: 0.0029   lr: 0.00061938  \n",
            "\u001b[32m[06/11 22:26:35 d2.utils.events]: \u001b[0m eta: 10:56:32  iter: 639  total_loss: 1.051  loss_cls: 0.2911  loss_box_reg: 0.3923  loss_mask: 0.2501  loss_rpn_cls: 0.03602  loss_rpn_loc: 0.06414    time: 12.6159  last_time: 11.8081  data_time: 0.0032  last_data_time: 0.0030   lr: 0.00063936  \n",
            "\u001b[32m[06/11 22:58:07 d2.utils.events]: \u001b[0m eta: 10:52:57  iter: 659  total_loss: 1.037  loss_cls: 0.2976  loss_box_reg: 0.3997  loss_mask: 0.2567  loss_rpn_cls: 0.03559  loss_rpn_loc: 0.05787    time: 12.5939  last_time: 12.1021  data_time: 0.0031  last_data_time: 0.0027   lr: 0.00065934  \n",
            "\u001b[32m[06/11 23:02:00 d2.utils.events]: \u001b[0m eta: 10:49:13  iter: 679  total_loss: 1.046  loss_cls: 0.2981  loss_box_reg: 0.4029  loss_mask: 0.2495  loss_rpn_cls: 0.03409  loss_rpn_loc: 0.05233    time: 12.5653  last_time: 10.4450  data_time: 0.0031  last_data_time: 0.0030   lr: 0.00067932  \n",
            "\u001b[32m[06/11 23:06:02 d2.utils.events]: \u001b[0m eta: 10:46:09  iter: 699  total_loss: 1.026  loss_cls: 0.2836  loss_box_reg: 0.4077  loss_mask: 0.2541  loss_rpn_cls: 0.03252  loss_rpn_loc: 0.0539    time: 12.5529  last_time: 11.8964  data_time: 0.0030  last_data_time: 0.0031   lr: 0.0006993  \n",
            "\u001b[32m[06/11 23:10:03 d2.utils.events]: \u001b[0m eta: 10:42:42  iter: 719  total_loss: 1.014  loss_cls: 0.2846  loss_box_reg: 0.3865  loss_mask: 0.2489  loss_rpn_cls: 0.03777  loss_rpn_loc: 0.06047    time: 12.5385  last_time: 11.4133  data_time: 0.0033  last_data_time: 0.0034   lr: 0.00071928  \n",
            "\u001b[32m[06/11 23:14:01 d2.utils.events]: \u001b[0m eta: 10:39:20  iter: 739  total_loss: 0.9938  loss_cls: 0.2705  loss_box_reg: 0.3559  loss_mask: 0.2422  loss_rpn_cls: 0.03204  loss_rpn_loc: 0.04938    time: 12.5208  last_time: 11.8362  data_time: 0.0031  last_data_time: 0.0025   lr: 0.00073926  \n",
            "\u001b[32m[06/11 23:17:51 d2.utils.events]: \u001b[0m eta: 10:35:24  iter: 759  total_loss: 1.014  loss_cls: 0.3039  loss_box_reg: 0.3913  loss_mask: 0.2397  loss_rpn_cls: 0.03763  loss_rpn_loc: 0.05606    time: 12.4938  last_time: 11.0862  data_time: 0.0030  last_data_time: 0.0029   lr: 0.00075924  \n",
            "\u001b[32m[06/11 23:21:47 d2.utils.events]: \u001b[0m eta: 10:32:02  iter: 779  total_loss: 0.9784  loss_cls: 0.2643  loss_box_reg: 0.3601  loss_mask: 0.2332  loss_rpn_cls: 0.03275  loss_rpn_loc: 0.05416    time: 12.4765  last_time: 11.7109  data_time: 0.0029  last_data_time: 0.0025   lr: 0.00077922  \n",
            "\u001b[32m[06/11 23:25:44 d2.utils.events]: \u001b[0m eta: 10:28:17  iter: 799  total_loss: 0.9494  loss_cls: 0.275  loss_box_reg: 0.3598  loss_mask: 0.2421  loss_rpn_cls: 0.02737  loss_rpn_loc: 0.0528    time: 12.4610  last_time: 11.9961  data_time: 0.0029  last_data_time: 0.0028   lr: 0.0007992  \n",
            "\u001b[32m[06/11 23:29:37 d2.utils.events]: \u001b[0m eta: 10:24:37  iter: 819  total_loss: 0.9199  loss_cls: 0.2642  loss_box_reg: 0.3559  loss_mask: 0.2238  loss_rpn_cls: 0.02872  loss_rpn_loc: 0.05686    time: 12.4411  last_time: 11.9273  data_time: 0.0029  last_data_time: 0.0030   lr: 0.00081918  \n",
            "\u001b[32m[06/11 23:33:38 d2.utils.events]: \u001b[0m eta: 10:21:31  iter: 839  total_loss: 0.9349  loss_cls: 0.2553  loss_box_reg: 0.3576  loss_mask: 0.2382  loss_rpn_cls: 0.03106  loss_rpn_loc: 0.05364    time: 12.4313  last_time: 11.8321  data_time: 0.0030  last_data_time: 0.0035   lr: 0.00083916  \n",
            "\u001b[32m[06/11 23:37:31 d2.utils.events]: \u001b[0m eta: 10:17:27  iter: 859  total_loss: 0.9106  loss_cls: 0.2512  loss_box_reg: 0.3644  loss_mask: 0.234  loss_rpn_cls: 0.02533  loss_rpn_loc: 0.04813    time: 12.4128  last_time: 11.9972  data_time: 0.0030  last_data_time: 0.0026   lr: 0.00085914  \n",
            "\u001b[32m[06/11 23:41:25 d2.utils.events]: \u001b[0m eta: 10:13:46  iter: 879  total_loss: 0.9608  loss_cls: 0.2708  loss_box_reg: 0.365  loss_mask: 0.2333  loss_rpn_cls: 0.03037  loss_rpn_loc: 0.05163    time: 12.3967  last_time: 10.9314  data_time: 0.0030  last_data_time: 0.0035   lr: 0.00087912  \n",
            "\u001b[32m[06/11 23:45:34 d2.utils.events]: \u001b[0m eta: 10:10:16  iter: 899  total_loss: 0.9295  loss_cls: 0.2387  loss_box_reg: 0.3503  loss_mask: 0.2295  loss_rpn_cls: 0.02661  loss_rpn_loc: 0.04955    time: 12.3979  last_time: 14.4264  data_time: 0.0032  last_data_time: 0.0034   lr: 0.0008991  \n",
            "\u001b[32m[06/11 23:49:33 d2.utils.events]: \u001b[0m eta: 10:06:32  iter: 919  total_loss: 0.9054  loss_cls: 0.2394  loss_box_reg: 0.3513  loss_mask: 0.2287  loss_rpn_cls: 0.0233  loss_rpn_loc: 0.0529    time: 12.3888  last_time: 12.1654  data_time: 0.0034  last_data_time: 0.0029   lr: 0.00091908  \n",
            "\u001b[32m[06/11 23:53:27 d2.utils.events]: \u001b[0m eta: 10:02:26  iter: 939  total_loss: 0.8345  loss_cls: 0.2267  loss_box_reg: 0.3157  loss_mask: 0.2171  loss_rpn_cls: 0.02503  loss_rpn_loc: 0.04817    time: 12.3738  last_time: 11.5074  data_time: 0.0030  last_data_time: 0.0027   lr: 0.00093906  \n",
            "\u001b[32m[06/11 23:57:13 d2.utils.events]: \u001b[0m eta: 9:58:06  iter: 959  total_loss: 0.8977  loss_cls: 0.2456  loss_box_reg: 0.3426  loss_mask: 0.2267  loss_rpn_cls: 0.02629  loss_rpn_loc: 0.05234    time: 12.3514  last_time: 11.8208  data_time: 0.0031  last_data_time: 0.0027   lr: 0.00095904  \n",
            "\u001b[32m[06/12 00:01:08 d2.utils.events]: \u001b[0m eta: 9:54:10  iter: 979  total_loss: 0.834  loss_cls: 0.2261  loss_box_reg: 0.3224  loss_mask: 0.2169  loss_rpn_cls: 0.02313  loss_rpn_loc: 0.04376    time: 12.3389  last_time: 11.5238  data_time: 0.0029  last_data_time: 0.0027   lr: 0.00097902  \n",
            "\u001b[32m[06/12 00:05:05 d2.utils.events]: \u001b[0m eta: 9:50:14  iter: 999  total_loss: 0.8767  loss_cls: 0.2411  loss_box_reg: 0.3459  loss_mask: 0.2246  loss_rpn_cls: 0.02432  loss_rpn_loc: 0.05109    time: 12.3289  last_time: 11.7450  data_time: 0.0030  last_data_time: 0.0032   lr: 0.000999  \n",
            "\u001b[32m[06/12 00:08:54 d2.utils.events]: \u001b[0m eta: 9:46:17  iter: 1019  total_loss: 0.8366  loss_cls: 0.2149  loss_box_reg: 0.3254  loss_mask: 0.2196  loss_rpn_cls: 0.01987  loss_rpn_loc: 0.0472    time: 12.3120  last_time: 11.4742  data_time: 0.0028  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 00:12:46 d2.utils.events]: \u001b[0m eta: 9:42:42  iter: 1039  total_loss: 0.8401  loss_cls: 0.2258  loss_box_reg: 0.3344  loss_mask: 0.222  loss_rpn_cls: 0.02352  loss_rpn_loc: 0.0475    time: 12.2987  last_time: 10.6888  data_time: 0.0030  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 00:16:36 d2.utils.events]: \u001b[0m eta: 9:38:54  iter: 1059  total_loss: 0.8093  loss_cls: 0.218  loss_box_reg: 0.3095  loss_mask: 0.2168  loss_rpn_cls: 0.02401  loss_rpn_loc: 0.04772    time: 12.2836  last_time: 11.7400  data_time: 0.0031  last_data_time: 0.0040   lr: 0.001  \n",
            "\u001b[32m[06/12 00:20:44 d2.utils.events]: \u001b[0m eta: 9:35:24  iter: 1079  total_loss: 0.7819  loss_cls: 0.2018  loss_box_reg: 0.3016  loss_mask: 0.2042  loss_rpn_cls: 0.01946  loss_rpn_loc: 0.04957    time: 12.2857  last_time: 12.0666  data_time: 0.0035  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 00:24:34 d2.utils.events]: \u001b[0m eta: 9:30:37  iter: 1099  total_loss: 0.7193  loss_cls: 0.1965  loss_box_reg: 0.2841  loss_mask: 0.2069  loss_rpn_cls: 0.01643  loss_rpn_loc: 0.04139    time: 12.2710  last_time: 11.5358  data_time: 0.0031  last_data_time: 0.0036   lr: 0.001  \n",
            "\u001b[32m[06/12 00:28:23 d2.utils.events]: \u001b[0m eta: 9:25:38  iter: 1119  total_loss: 0.8197  loss_cls: 0.2118  loss_box_reg: 0.3247  loss_mask: 0.2113  loss_rpn_cls: 0.01982  loss_rpn_loc: 0.05091    time: 12.2566  last_time: 11.5342  data_time: 0.0030  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 00:32:08 d2.utils.events]: \u001b[0m eta: 9:20:59  iter: 1139  total_loss: 0.7899  loss_cls: 0.2027  loss_box_reg: 0.3159  loss_mask: 0.2168  loss_rpn_cls: 0.01804  loss_rpn_loc: 0.04757    time: 12.2389  last_time: 11.5920  data_time: 0.0030  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 00:35:54 d2.utils.events]: \u001b[0m eta: 9:16:38  iter: 1159  total_loss: 0.8082  loss_cls: 0.2049  loss_box_reg: 0.3048  loss_mask: 0.2105  loss_rpn_cls: 0.01919  loss_rpn_loc: 0.0482    time: 12.2222  last_time: 11.4454  data_time: 0.0029  last_data_time: 0.0026   lr: 0.001  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/12 00:39:40 d2.utils.events]: \u001b[0m eta: 9:12:33  iter: 1179  total_loss: 0.7718  loss_cls: 0.1974  loss_box_reg: 0.2937  loss_mask: 0.2028  loss_rpn_cls: 0.01699  loss_rpn_loc: 0.04934    time: 12.2069  last_time: 11.7650  data_time: 0.0030  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 00:43:25 d2.utils.events]: \u001b[0m eta: 9:07:56  iter: 1199  total_loss: 0.7567  loss_cls: 0.197  loss_box_reg: 0.3001  loss_mask: 0.2036  loss_rpn_cls: 0.01561  loss_rpn_loc: 0.04875    time: 12.1905  last_time: 10.7723  data_time: 0.0032  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 00:47:11 d2.utils.events]: \u001b[0m eta: 9:03:40  iter: 1219  total_loss: 0.7274  loss_cls: 0.1783  loss_box_reg: 0.2883  loss_mask: 0.2013  loss_rpn_cls: 0.01901  loss_rpn_loc: 0.04452    time: 12.1763  last_time: 11.6450  data_time: 0.0029  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 00:50:57 d2.utils.events]: \u001b[0m eta: 8:59:39  iter: 1239  total_loss: 0.7317  loss_cls: 0.1792  loss_box_reg: 0.2882  loss_mask: 0.2009  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.04388    time: 12.1622  last_time: 11.5496  data_time: 0.0028  last_data_time: 0.0038   lr: 0.001  \n",
            "\u001b[32m[06/12 00:54:43 d2.utils.events]: \u001b[0m eta: 8:55:11  iter: 1259  total_loss: 0.7286  loss_cls: 0.1846  loss_box_reg: 0.2839  loss_mask: 0.1995  loss_rpn_cls: 0.01677  loss_rpn_loc: 0.0452    time: 12.1485  last_time: 10.8686  data_time: 0.0030  last_data_time: 0.0037   lr: 0.001  \n",
            "\u001b[32m[06/12 00:58:30 d2.utils.events]: \u001b[0m eta: 8:50:58  iter: 1279  total_loss: 0.7059  loss_cls: 0.1739  loss_box_reg: 0.282  loss_mask: 0.1972  loss_rpn_cls: 0.01489  loss_rpn_loc: 0.04161    time: 12.1355  last_time: 11.3316  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 01:02:21 d2.utils.events]: \u001b[0m eta: 8:46:47  iter: 1299  total_loss: 0.7586  loss_cls: 0.2028  loss_box_reg: 0.2948  loss_mask: 0.2051  loss_rpn_cls: 0.01639  loss_rpn_loc: 0.04979    time: 12.1264  last_time: 11.4845  data_time: 0.0030  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 01:06:07 d2.utils.events]: \u001b[0m eta: 8:42:38  iter: 1319  total_loss: 0.7189  loss_cls: 0.1639  loss_box_reg: 0.2748  loss_mask: 0.2032  loss_rpn_cls: 0.01613  loss_rpn_loc: 0.04709    time: 12.1143  last_time: 10.1436  data_time: 0.0030  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 01:09:54 d2.utils.events]: \u001b[0m eta: 8:38:03  iter: 1339  total_loss: 0.7013  loss_cls: 0.162  loss_box_reg: 0.2751  loss_mask: 0.1942  loss_rpn_cls: 0.01615  loss_rpn_loc: 0.04535    time: 12.1025  last_time: 11.7107  data_time: 0.0029  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 01:13:41 d2.utils.events]: \u001b[0m eta: 8:33:32  iter: 1359  total_loss: 0.6708  loss_cls: 0.1569  loss_box_reg: 0.2661  loss_mask: 0.194  loss_rpn_cls: 0.01337  loss_rpn_loc: 0.04104    time: 12.0917  last_time: 11.4277  data_time: 0.0030  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 01:17:28 d2.utils.events]: \u001b[0m eta: 8:29:25  iter: 1379  total_loss: 0.6619  loss_cls: 0.1641  loss_box_reg: 0.2592  loss_mask: 0.1868  loss_rpn_cls: 0.01317  loss_rpn_loc: 0.04078    time: 12.0810  last_time: 10.8000  data_time: 0.0032  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 01:21:14 d2.utils.events]: \u001b[0m eta: 8:25:22  iter: 1399  total_loss: 0.693  loss_cls: 0.1713  loss_box_reg: 0.2705  loss_mask: 0.198  loss_rpn_cls: 0.01249  loss_rpn_loc: 0.04463    time: 12.0699  last_time: 10.7987  data_time: 0.0030  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 01:25:02 d2.utils.events]: \u001b[0m eta: 8:20:48  iter: 1419  total_loss: 0.6652  loss_cls: 0.1484  loss_box_reg: 0.2537  loss_mask: 0.1925  loss_rpn_cls: 0.01295  loss_rpn_loc: 0.04244    time: 12.0606  last_time: 11.4559  data_time: 0.0031  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 01:28:48 d2.utils.events]: \u001b[0m eta: 8:16:24  iter: 1439  total_loss: 0.6839  loss_cls: 0.1621  loss_box_reg: 0.2641  loss_mask: 0.1907  loss_rpn_cls: 0.01205  loss_rpn_loc: 0.04411    time: 12.0498  last_time: 11.6399  data_time: 0.0030  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 01:32:36 d2.utils.events]: \u001b[0m eta: 8:12:14  iter: 1459  total_loss: 0.6404  loss_cls: 0.1492  loss_box_reg: 0.2437  loss_mask: 0.1849  loss_rpn_cls: 0.01113  loss_rpn_loc: 0.04214    time: 12.0410  last_time: 11.4298  data_time: 0.0031  last_data_time: 0.0043   lr: 0.001  \n",
            "\u001b[32m[06/12 01:36:18 d2.utils.events]: \u001b[0m eta: 8:08:11  iter: 1479  total_loss: 0.6314  loss_cls: 0.1398  loss_box_reg: 0.2454  loss_mask: 0.1817  loss_rpn_cls: 0.01191  loss_rpn_loc: 0.04029    time: 12.0279  last_time: 9.8018  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 01:40:04 d2.utils.events]: \u001b[0m eta: 8:03:43  iter: 1499  total_loss: 0.6795  loss_cls: 0.1544  loss_box_reg: 0.2658  loss_mask: 0.1939  loss_rpn_cls: 0.0128  loss_rpn_loc: 0.04579    time: 12.0183  last_time: 11.2551  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 01:43:51 d2.utils.events]: \u001b[0m eta: 7:59:08  iter: 1519  total_loss: 0.6373  loss_cls: 0.1407  loss_box_reg: 0.2518  loss_mask: 0.1876  loss_rpn_cls: 0.00963  loss_rpn_loc: 0.04271    time: 12.0095  last_time: 11.2621  data_time: 0.0029  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 01:47:39 d2.utils.events]: \u001b[0m eta: 7:54:56  iter: 1539  total_loss: 0.5847  loss_cls: 0.1298  loss_box_reg: 0.2388  loss_mask: 0.1803  loss_rpn_cls: 0.01044  loss_rpn_loc: 0.0394    time: 12.0014  last_time: 11.4841  data_time: 0.0031  last_data_time: 0.0039   lr: 0.001  \n",
            "\u001b[32m[06/12 01:51:25 d2.utils.events]: \u001b[0m eta: 7:50:43  iter: 1559  total_loss: 0.6281  loss_cls: 0.1432  loss_box_reg: 0.2486  loss_mask: 0.1823  loss_rpn_cls: 0.00955  loss_rpn_loc: 0.03983    time: 11.9929  last_time: 11.4926  data_time: 0.0030  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 01:55:10 d2.utils.events]: \u001b[0m eta: 7:46:31  iter: 1579  total_loss: 0.6209  loss_cls: 0.1444  loss_box_reg: 0.2457  loss_mask: 0.1863  loss_rpn_cls: 0.01065  loss_rpn_loc: 0.0386    time: 11.9832  last_time: 11.4786  data_time: 0.0030  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 01:58:54 d2.utils.events]: \u001b[0m eta: 7:42:31  iter: 1599  total_loss: 0.6237  loss_cls: 0.1431  loss_box_reg: 0.2464  loss_mask: 0.1795  loss_rpn_cls: 0.01032  loss_rpn_loc: 0.04595    time: 11.9731  last_time: 11.1828  data_time: 0.0031  last_data_time: 0.0023   lr: 0.001  \n",
            "\u001b[32m[06/12 02:02:41 d2.utils.events]: \u001b[0m eta: 7:38:18  iter: 1619  total_loss: 0.6267  loss_cls: 0.1367  loss_box_reg: 0.2399  loss_mask: 0.1859  loss_rpn_cls: 0.008456  loss_rpn_loc: 0.04181    time: 11.9656  last_time: 11.2454  data_time: 0.0030  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 02:06:28 d2.utils.events]: \u001b[0m eta: 7:34:15  iter: 1639  total_loss: 0.6223  loss_cls: 0.1426  loss_box_reg: 0.2437  loss_mask: 0.1788  loss_rpn_cls: 0.009429  loss_rpn_loc: 0.04115    time: 11.9584  last_time: 11.5941  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 02:10:14 d2.utils.events]: \u001b[0m eta: 7:30:00  iter: 1659  total_loss: 0.5951  loss_cls: 0.1299  loss_box_reg: 0.2342  loss_mask: 0.1773  loss_rpn_cls: 0.009957  loss_rpn_loc: 0.03511    time: 11.9500  last_time: 11.5825  data_time: 0.0031  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 02:14:01 d2.utils.events]: \u001b[0m eta: 7:26:01  iter: 1679  total_loss: 0.6026  loss_cls: 0.1317  loss_box_reg: 0.2378  loss_mask: 0.1802  loss_rpn_cls: 0.009522  loss_rpn_loc: 0.03649    time: 11.9430  last_time: 11.6634  data_time: 0.0029  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 02:17:45 d2.utils.events]: \u001b[0m eta: 7:22:01  iter: 1699  total_loss: 0.5954  loss_cls: 0.1448  loss_box_reg: 0.2307  loss_mask: 0.1752  loss_rpn_cls: 0.01041  loss_rpn_loc: 0.04161    time: 11.9344  last_time: 10.8777  data_time: 0.0031  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 02:21:29 d2.utils.events]: \u001b[0m eta: 7:17:57  iter: 1719  total_loss: 0.5952  loss_cls: 0.1243  loss_box_reg: 0.2287  loss_mask: 0.179  loss_rpn_cls: 0.009624  loss_rpn_loc: 0.04199    time: 11.9257  last_time: 10.8501  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 02:25:15 d2.utils.events]: \u001b[0m eta: 7:13:44  iter: 1739  total_loss: 0.5667  loss_cls: 0.1237  loss_box_reg: 0.222  loss_mask: 0.1773  loss_rpn_cls: 0.00729  loss_rpn_loc: 0.04185    time: 11.9186  last_time: 11.4586  data_time: 0.0029  last_data_time: 0.0027   lr: 0.001  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/12 02:29:00 d2.utils.events]: \u001b[0m eta: 7:09:44  iter: 1759  total_loss: 0.5267  loss_cls: 0.1063  loss_box_reg: 0.2172  loss_mask: 0.168  loss_rpn_cls: 0.006705  loss_rpn_loc: 0.03491    time: 11.9107  last_time: 11.3523  data_time: 0.0029  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 02:32:47 d2.utils.events]: \u001b[0m eta: 7:05:33  iter: 1779  total_loss: 0.6227  loss_cls: 0.134  loss_box_reg: 0.249  loss_mask: 0.1886  loss_rpn_cls: 0.008036  loss_rpn_loc: 0.04369    time: 11.9045  last_time: 11.3213  data_time: 0.0031  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 02:36:32 d2.utils.events]: \u001b[0m eta: 7:01:38  iter: 1799  total_loss: 0.5575  loss_cls: 0.119  loss_box_reg: 0.2241  loss_mask: 0.1651  loss_rpn_cls: 0.008774  loss_rpn_loc: 0.037    time: 11.8973  last_time: 11.6986  data_time: 0.0032  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 02:40:18 d2.utils.events]: \u001b[0m eta: 6:57:36  iter: 1819  total_loss: 0.6149  loss_cls: 0.1507  loss_box_reg: 0.2393  loss_mask: 0.1794  loss_rpn_cls: 0.01109  loss_rpn_loc: 0.04381    time: 11.8909  last_time: 10.7276  data_time: 0.0031  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 02:44:04 d2.utils.events]: \u001b[0m eta: 6:53:30  iter: 1839  total_loss: 0.5859  loss_cls: 0.1347  loss_box_reg: 0.2282  loss_mask: 0.1714  loss_rpn_cls: 0.007248  loss_rpn_loc: 0.03569    time: 11.8841  last_time: 11.2024  data_time: 0.0031  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 02:47:51 d2.utils.events]: \u001b[0m eta: 6:49:34  iter: 1859  total_loss: 0.5271  loss_cls: 0.1139  loss_box_reg: 0.2099  loss_mask: 0.1733  loss_rpn_cls: 0.007772  loss_rpn_loc: 0.03436    time: 11.8787  last_time: 11.2999  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 02:51:38 d2.utils.events]: \u001b[0m eta: 6:45:31  iter: 1879  total_loss: 0.5532  loss_cls: 0.1323  loss_box_reg: 0.2189  loss_mask: 0.1654  loss_rpn_cls: 0.007457  loss_rpn_loc: 0.03598    time: 11.8730  last_time: 10.8621  data_time: 0.0029  last_data_time: 0.0039   lr: 0.001  \n",
            "\u001b[32m[06/12 02:55:24 d2.utils.events]: \u001b[0m eta: 6:41:28  iter: 1899  total_loss: 0.5317  loss_cls: 0.1061  loss_box_reg: 0.2128  loss_mask: 0.1688  loss_rpn_cls: 0.007499  loss_rpn_loc: 0.03348    time: 11.8670  last_time: 11.4380  data_time: 0.0033  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 02:59:10 d2.utils.events]: \u001b[0m eta: 6:37:30  iter: 1919  total_loss: 0.5319  loss_cls: 0.1128  loss_box_reg: 0.2117  loss_mask: 0.1744  loss_rpn_cls: 0.006029  loss_rpn_loc: 0.03841    time: 11.8610  last_time: 11.3749  data_time: 0.0031  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 03:02:56 d2.utils.events]: \u001b[0m eta: 6:33:36  iter: 1939  total_loss: 0.5589  loss_cls: 0.1213  loss_box_reg: 0.2129  loss_mask: 0.1683  loss_rpn_cls: 0.006136  loss_rpn_loc: 0.03571    time: 11.8553  last_time: 11.7115  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 03:06:41 d2.utils.events]: \u001b[0m eta: 6:29:45  iter: 1959  total_loss: 0.5254  loss_cls: 0.1082  loss_box_reg: 0.2054  loss_mask: 0.1688  loss_rpn_cls: 0.005353  loss_rpn_loc: 0.03651    time: 11.8491  last_time: 11.3125  data_time: 0.0032  last_data_time: 0.0039   lr: 0.001  \n",
            "\u001b[32m[06/12 03:10:28 d2.utils.events]: \u001b[0m eta: 6:25:45  iter: 1979  total_loss: 0.5171  loss_cls: 0.1052  loss_box_reg: 0.2105  loss_mask: 0.1648  loss_rpn_cls: 0.007716  loss_rpn_loc: 0.03562    time: 11.8441  last_time: 10.8325  data_time: 0.0029  last_data_time: 0.0023   lr: 0.001  \n",
            "\u001b[32m[06/12 03:14:16 d2.utils.events]: \u001b[0m eta: 6:21:41  iter: 1999  total_loss: 0.5274  loss_cls: 0.1091  loss_box_reg: 0.2108  loss_mask: 0.1671  loss_rpn_cls: 0.004836  loss_rpn_loc: 0.03564    time: 11.8395  last_time: 11.3851  data_time: 0.0030  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 03:18:01 d2.utils.events]: \u001b[0m eta: 6:17:46  iter: 2019  total_loss: 0.507  loss_cls: 0.1065  loss_box_reg: 0.203  loss_mask: 0.1677  loss_rpn_cls: 0.006916  loss_rpn_loc: 0.03271    time: 11.8339  last_time: 11.4346  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 03:21:47 d2.utils.events]: \u001b[0m eta: 6:13:56  iter: 2039  total_loss: 0.5015  loss_cls: 0.09535  loss_box_reg: 0.2044  loss_mask: 0.1623  loss_rpn_cls: 0.006351  loss_rpn_loc: 0.03528    time: 11.8283  last_time: 11.4744  data_time: 0.0029  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 03:25:34 d2.utils.events]: \u001b[0m eta: 6:10:07  iter: 2059  total_loss: 0.5639  loss_cls: 0.1212  loss_box_reg: 0.2226  loss_mask: 0.1726  loss_rpn_cls: 0.006305  loss_rpn_loc: 0.03959    time: 11.8241  last_time: 11.4476  data_time: 0.0031  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 03:29:19 d2.utils.events]: \u001b[0m eta: 6:06:12  iter: 2079  total_loss: 0.4917  loss_cls: 0.1025  loss_box_reg: 0.1886  loss_mask: 0.1543  loss_rpn_cls: 0.005567  loss_rpn_loc: 0.0341    time: 11.8185  last_time: 11.6966  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 03:33:06 d2.utils.events]: \u001b[0m eta: 6:02:13  iter: 2099  total_loss: 0.5133  loss_cls: 0.1042  loss_box_reg: 0.2057  loss_mask: 0.1613  loss_rpn_cls: 0.006022  loss_rpn_loc: 0.03391    time: 11.8137  last_time: 11.5045  data_time: 0.0030  last_data_time: 0.0023   lr: 0.001  \n",
            "\u001b[32m[06/12 03:36:51 d2.utils.events]: \u001b[0m eta: 5:58:16  iter: 2119  total_loss: 0.503  loss_cls: 0.1053  loss_box_reg: 0.1974  loss_mask: 0.1701  loss_rpn_cls: 0.005152  loss_rpn_loc: 0.036    time: 11.8084  last_time: 11.4106  data_time: 0.0030  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 03:40:36 d2.utils.events]: \u001b[0m eta: 5:54:25  iter: 2139  total_loss: 0.544  loss_cls: 0.1142  loss_box_reg: 0.208  loss_mask: 0.1659  loss_rpn_cls: 0.005921  loss_rpn_loc: 0.03847    time: 11.8035  last_time: 11.4579  data_time: 0.0031  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 03:44:23 d2.utils.events]: \u001b[0m eta: 5:50:36  iter: 2159  total_loss: 0.5008  loss_cls: 0.1062  loss_box_reg: 0.2004  loss_mask: 0.1638  loss_rpn_cls: 0.005361  loss_rpn_loc: 0.03366    time: 11.7992  last_time: 11.7325  data_time: 0.0030  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 03:48:08 d2.utils.events]: \u001b[0m eta: 5:46:41  iter: 2179  total_loss: 0.5169  loss_cls: 0.114  loss_box_reg: 0.1994  loss_mask: 0.1666  loss_rpn_cls: 0.005326  loss_rpn_loc: 0.0379    time: 11.7943  last_time: 10.8349  data_time: 0.0031  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 03:51:54 d2.utils.events]: \u001b[0m eta: 5:42:49  iter: 2199  total_loss: 0.4897  loss_cls: 0.09499  loss_box_reg: 0.1961  loss_mask: 0.1624  loss_rpn_cls: 0.004166  loss_rpn_loc: 0.03224    time: 11.7895  last_time: 10.7713  data_time: 0.0031  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 03:55:41 d2.utils.events]: \u001b[0m eta: 5:39:03  iter: 2219  total_loss: 0.5168  loss_cls: 0.1047  loss_box_reg: 0.2018  loss_mask: 0.1665  loss_rpn_cls: 0.006457  loss_rpn_loc: 0.03659    time: 11.7854  last_time: 11.4961  data_time: 0.0031  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 03:59:28 d2.utils.events]: \u001b[0m eta: 5:35:15  iter: 2239  total_loss: 0.4968  loss_cls: 0.09748  loss_box_reg: 0.1927  loss_mask: 0.1616  loss_rpn_cls: 0.005071  loss_rpn_loc: 0.0323    time: 11.7817  last_time: 11.5516  data_time: 0.0029  last_data_time: 0.0033   lr: 0.001  \n",
            "\u001b[32m[06/12 04:03:16 d2.utils.events]: \u001b[0m eta: 5:31:28  iter: 2259  total_loss: 0.4945  loss_cls: 0.1077  loss_box_reg: 0.193  loss_mask: 0.158  loss_rpn_cls: 0.005  loss_rpn_loc: 0.03063    time: 11.7785  last_time: 11.5717  data_time: 0.0032  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 04:07:05 d2.utils.events]: \u001b[0m eta: 5:27:41  iter: 2279  total_loss: 0.4846  loss_cls: 0.08587  loss_box_reg: 0.1975  loss_mask: 0.1605  loss_rpn_cls: 0.004972  loss_rpn_loc: 0.03569    time: 11.7755  last_time: 11.3680  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 04:10:50 d2.utils.events]: \u001b[0m eta: 5:23:46  iter: 2299  total_loss: 0.5016  loss_cls: 0.1058  loss_box_reg: 0.204  loss_mask: 0.1594  loss_rpn_cls: 0.005078  loss_rpn_loc: 0.03483    time: 11.7707  last_time: 10.8142  data_time: 0.0030  last_data_time: 0.0037   lr: 0.001  \n",
            "\u001b[32m[06/12 04:14:34 d2.utils.events]: \u001b[0m eta: 5:19:54  iter: 2319  total_loss: 0.483  loss_cls: 0.09073  loss_box_reg: 0.1941  loss_mask: 0.1516  loss_rpn_cls: 0.005542  loss_rpn_loc: 0.03406    time: 11.7660  last_time: 10.8729  data_time: 0.0032  last_data_time: 0.0028   lr: 0.001  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/12 04:18:21 d2.utils.events]: \u001b[0m eta: 5:16:05  iter: 2339  total_loss: 0.4624  loss_cls: 0.09396  loss_box_reg: 0.1828  loss_mask: 0.1555  loss_rpn_cls: 0.003818  loss_rpn_loc: 0.03079    time: 11.7622  last_time: 11.3430  data_time: 0.0030  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 04:22:06 d2.utils.events]: \u001b[0m eta: 5:12:16  iter: 2359  total_loss: 0.501  loss_cls: 0.1052  loss_box_reg: 0.1872  loss_mask: 0.1583  loss_rpn_cls: 0.005002  loss_rpn_loc: 0.03601    time: 11.7578  last_time: 11.3820  data_time: 0.0030  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 04:25:51 d2.utils.events]: \u001b[0m eta: 5:08:28  iter: 2379  total_loss: 0.4827  loss_cls: 0.09293  loss_box_reg: 0.1865  loss_mask: 0.1568  loss_rpn_cls: 0.004713  loss_rpn_loc: 0.03401    time: 11.7536  last_time: 11.5562  data_time: 0.0030  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 04:29:38 d2.utils.events]: \u001b[0m eta: 5:04:42  iter: 2399  total_loss: 0.5337  loss_cls: 0.1059  loss_box_reg: 0.205  loss_mask: 0.1678  loss_rpn_cls: 0.005602  loss_rpn_loc: 0.03751    time: 11.7505  last_time: 11.3771  data_time: 0.0031  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 04:33:27 d2.utils.events]: \u001b[0m eta: 5:00:57  iter: 2419  total_loss: 0.5097  loss_cls: 0.1025  loss_box_reg: 0.1981  loss_mask: 0.1643  loss_rpn_cls: 0.004146  loss_rpn_loc: 0.0374    time: 11.7478  last_time: 11.5591  data_time: 0.0030  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 04:37:12 d2.utils.events]: \u001b[0m eta: 4:57:09  iter: 2439  total_loss: 0.4718  loss_cls: 0.0896  loss_box_reg: 0.1833  loss_mask: 0.1534  loss_rpn_cls: 0.003927  loss_rpn_loc: 0.02927    time: 11.7438  last_time: 11.5206  data_time: 0.0032  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 04:40:55 d2.utils.events]: \u001b[0m eta: 4:53:16  iter: 2459  total_loss: 0.4793  loss_cls: 0.08732  loss_box_reg: 0.1843  loss_mask: 0.1578  loss_rpn_cls: 0.005162  loss_rpn_loc: 0.03408    time: 11.7388  last_time: 11.7639  data_time: 0.0031  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 04:44:43 d2.utils.events]: \u001b[0m eta: 4:49:32  iter: 2479  total_loss: 0.453  loss_cls: 0.08484  loss_box_reg: 0.1798  loss_mask: 0.1566  loss_rpn_cls: 0.003675  loss_rpn_loc: 0.03022    time: 11.7362  last_time: 10.8243  data_time: 0.0032  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 04:48:29 d2.utils.events]: \u001b[0m eta: 4:45:39  iter: 2499  total_loss: 0.4763  loss_cls: 0.09686  loss_box_reg: 0.1886  loss_mask: 0.1539  loss_rpn_cls: 0.004277  loss_rpn_loc: 0.03314    time: 11.7327  last_time: 11.4482  data_time: 0.0029  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 04:52:14 d2.utils.events]: \u001b[0m eta: 4:41:50  iter: 2519  total_loss: 0.5107  loss_cls: 0.1024  loss_box_reg: 0.2003  loss_mask: 0.1614  loss_rpn_cls: 0.004571  loss_rpn_loc: 0.03722    time: 11.7289  last_time: 10.7377  data_time: 0.0031  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 04:56:00 d2.utils.events]: \u001b[0m eta: 4:37:57  iter: 2539  total_loss: 0.4454  loss_cls: 0.08131  loss_box_reg: 0.1776  loss_mask: 0.1536  loss_rpn_cls: 0.003213  loss_rpn_loc: 0.03023    time: 11.7255  last_time: 11.3259  data_time: 0.0031  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 04:59:49 d2.utils.events]: \u001b[0m eta: 4:34:08  iter: 2559  total_loss: 0.4862  loss_cls: 0.102  loss_box_reg: 0.1857  loss_mask: 0.1612  loss_rpn_cls: 0.00428  loss_rpn_loc: 0.0369    time: 11.7233  last_time: 11.3947  data_time: 0.0032  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 05:03:34 d2.utils.events]: \u001b[0m eta: 4:30:17  iter: 2579  total_loss: 0.4355  loss_cls: 0.0802  loss_box_reg: 0.1678  loss_mask: 0.1494  loss_rpn_cls: 0.004239  loss_rpn_loc: 0.02655    time: 11.7196  last_time: 11.5904  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 05:07:21 d2.utils.events]: \u001b[0m eta: 4:26:31  iter: 2599  total_loss: 0.4911  loss_cls: 0.09556  loss_box_reg: 0.1876  loss_mask: 0.1592  loss_rpn_cls: 0.004205  loss_rpn_loc: 0.0357    time: 11.7167  last_time: 11.3291  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 05:11:06 d2.utils.events]: \u001b[0m eta: 4:22:42  iter: 2619  total_loss: 0.4517  loss_cls: 0.08418  loss_box_reg: 0.1754  loss_mask: 0.1497  loss_rpn_cls: 0.004556  loss_rpn_loc: 0.03075    time: 11.7133  last_time: 11.2669  data_time: 0.0028  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 05:14:53 d2.utils.events]: \u001b[0m eta: 4:18:51  iter: 2639  total_loss: 0.4545  loss_cls: 0.08446  loss_box_reg: 0.1861  loss_mask: 0.1533  loss_rpn_cls: 0.003185  loss_rpn_loc: 0.03384    time: 11.7104  last_time: 11.4127  data_time: 0.0033  last_data_time: 0.0042   lr: 0.001  \n",
            "\u001b[32m[06/12 05:18:38 d2.utils.events]: \u001b[0m eta: 4:15:04  iter: 2659  total_loss: 0.4721  loss_cls: 0.0964  loss_box_reg: 0.1831  loss_mask: 0.1519  loss_rpn_cls: 0.00388  loss_rpn_loc: 0.03542    time: 11.7069  last_time: 11.4201  data_time: 0.0031  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 05:22:22 d2.utils.events]: \u001b[0m eta: 4:11:15  iter: 2679  total_loss: 0.4546  loss_cls: 0.08937  loss_box_reg: 0.1781  loss_mask: 0.1511  loss_rpn_cls: 0.003049  loss_rpn_loc: 0.03163    time: 11.7032  last_time: 10.7921  data_time: 0.0031  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 05:26:09 d2.utils.events]: \u001b[0m eta: 4:07:29  iter: 2699  total_loss: 0.4397  loss_cls: 0.07841  loss_box_reg: 0.1681  loss_mask: 0.1477  loss_rpn_cls: 0.002486  loss_rpn_loc: 0.03307    time: 11.7006  last_time: 11.5866  data_time: 0.0031  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 05:29:57 d2.utils.events]: \u001b[0m eta: 4:03:41  iter: 2719  total_loss: 0.447  loss_cls: 0.08782  loss_box_reg: 0.1713  loss_mask: 0.1456  loss_rpn_cls: 0.003817  loss_rpn_loc: 0.03167    time: 11.6984  last_time: 11.3844  data_time: 0.0028  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 05:33:44 d2.utils.events]: \u001b[0m eta: 3:59:52  iter: 2739  total_loss: 0.4307  loss_cls: 0.07798  loss_box_reg: 0.1696  loss_mask: 0.1439  loss_rpn_cls: 0.004311  loss_rpn_loc: 0.02884    time: 11.6958  last_time: 10.7987  data_time: 0.0029  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 05:37:31 d2.utils.events]: \u001b[0m eta: 3:56:04  iter: 2759  total_loss: 0.443  loss_cls: 0.08361  loss_box_reg: 0.1724  loss_mask: 0.1519  loss_rpn_cls: 0.003038  loss_rpn_loc: 0.03246    time: 11.6932  last_time: 11.2976  data_time: 0.0030  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 05:41:15 d2.utils.events]: \u001b[0m eta: 3:52:15  iter: 2779  total_loss: 0.443  loss_cls: 0.08436  loss_box_reg: 0.1709  loss_mask: 0.1519  loss_rpn_cls: 0.003364  loss_rpn_loc: 0.03259    time: 11.6897  last_time: 11.5336  data_time: 0.0031  last_data_time: 0.0036   lr: 0.001  \n",
            "\u001b[32m[06/12 05:44:58 d2.utils.events]: \u001b[0m eta: 3:48:26  iter: 2799  total_loss: 0.4431  loss_cls: 0.07947  loss_box_reg: 0.1705  loss_mask: 0.1488  loss_rpn_cls: 0.004106  loss_rpn_loc: 0.03377    time: 11.6860  last_time: 11.6883  data_time: 0.0028  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 05:48:44 d2.utils.events]: \u001b[0m eta: 3:44:36  iter: 2819  total_loss: 0.4582  loss_cls: 0.08353  loss_box_reg: 0.1795  loss_mask: 0.15  loss_rpn_cls: 0.003027  loss_rpn_loc: 0.03215    time: 11.6832  last_time: 10.2306  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 05:52:31 d2.utils.events]: \u001b[0m eta: 3:40:49  iter: 2839  total_loss: 0.4331  loss_cls: 0.08111  loss_box_reg: 0.1749  loss_mask: 0.1458  loss_rpn_cls: 0.002821  loss_rpn_loc: 0.0329    time: 11.6808  last_time: 10.7572  data_time: 0.0030  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 05:56:19 d2.utils.events]: \u001b[0m eta: 3:37:00  iter: 2859  total_loss: 0.4175  loss_cls: 0.08139  loss_box_reg: 0.1631  loss_mask: 0.1421  loss_rpn_cls: 0.002895  loss_rpn_loc: 0.03004    time: 11.6788  last_time: 10.8694  data_time: 0.0031  last_data_time: 0.0036   lr: 0.001  \n",
            "\u001b[32m[06/12 06:00:06 d2.utils.events]: \u001b[0m eta: 3:33:11  iter: 2879  total_loss: 0.4518  loss_cls: 0.08645  loss_box_reg: 0.1697  loss_mask: 0.155  loss_rpn_cls: 0.00273  loss_rpn_loc: 0.03497    time: 11.6767  last_time: 11.3819  data_time: 0.0029  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 06:03:52 d2.utils.events]: \u001b[0m eta: 3:29:24  iter: 2899  total_loss: 0.4273  loss_cls: 0.08111  loss_box_reg: 0.1621  loss_mask: 0.1445  loss_rpn_cls: 0.002733  loss_rpn_loc: 0.03075    time: 11.6739  last_time: 11.7601  data_time: 0.0030  last_data_time: 0.0026   lr: 0.001  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/12 06:07:40 d2.utils.events]: \u001b[0m eta: 3:25:38  iter: 2919  total_loss: 0.4231  loss_cls: 0.07748  loss_box_reg: 0.1698  loss_mask: 0.1443  loss_rpn_cls: 0.00347  loss_rpn_loc: 0.02952    time: 11.6720  last_time: 11.4400  data_time: 0.0031  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 06:11:24 d2.utils.events]: \u001b[0m eta: 3:21:49  iter: 2939  total_loss: 0.4578  loss_cls: 0.08605  loss_box_reg: 0.1852  loss_mask: 0.1487  loss_rpn_cls: 0.003074  loss_rpn_loc: 0.03333    time: 11.6690  last_time: 11.5184  data_time: 0.0030  last_data_time: 0.0039   lr: 0.001  \n",
            "\u001b[32m[06/12 06:15:10 d2.utils.events]: \u001b[0m eta: 3:18:02  iter: 2959  total_loss: 0.4092  loss_cls: 0.07243  loss_box_reg: 0.1603  loss_mask: 0.138  loss_rpn_cls: 0.003095  loss_rpn_loc: 0.02787    time: 11.6665  last_time: 11.4710  data_time: 0.0030  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 06:18:52 d2.utils.events]: \u001b[0m eta: 3:14:13  iter: 2979  total_loss: 0.4062  loss_cls: 0.07852  loss_box_reg: 0.162  loss_mask: 0.139  loss_rpn_cls: 0.002557  loss_rpn_loc: 0.02853    time: 11.6626  last_time: 11.5592  data_time: 0.0030  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 06:22:39 d2.utils.events]: \u001b[0m eta: 3:10:26  iter: 2999  total_loss: 0.4252  loss_cls: 0.076  loss_box_reg: 0.1699  loss_mask: 0.1447  loss_rpn_cls: 0.002874  loss_rpn_loc: 0.03015    time: 11.6606  last_time: 11.6745  data_time: 0.0029  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 06:26:27 d2.utils.events]: \u001b[0m eta: 3:06:37  iter: 3019  total_loss: 0.4114  loss_cls: 0.07129  loss_box_reg: 0.1625  loss_mask: 0.1455  loss_rpn_cls: 0.002977  loss_rpn_loc: 0.03214    time: 11.6587  last_time: 11.3417  data_time: 0.0028  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 06:30:15 d2.utils.events]: \u001b[0m eta: 3:02:49  iter: 3039  total_loss: 0.4311  loss_cls: 0.07993  loss_box_reg: 0.1698  loss_mask: 0.1442  loss_rpn_cls: 0.002466  loss_rpn_loc: 0.02917    time: 11.6570  last_time: 11.3286  data_time: 0.0030  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 06:33:58 d2.utils.events]: \u001b[0m eta: 2:58:59  iter: 3059  total_loss: 0.4393  loss_cls: 0.08397  loss_box_reg: 0.1638  loss_mask: 0.147  loss_rpn_cls: 0.002439  loss_rpn_loc: 0.03189    time: 11.6539  last_time: 10.0328  data_time: 0.0030  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 06:37:44 d2.utils.events]: \u001b[0m eta: 2:55:12  iter: 3079  total_loss: 0.4179  loss_cls: 0.07615  loss_box_reg: 0.1653  loss_mask: 0.1449  loss_rpn_cls: 0.00352  loss_rpn_loc: 0.03057    time: 11.6515  last_time: 11.4909  data_time: 0.0031  last_data_time: 0.0051   lr: 0.001  \n",
            "\u001b[32m[06/12 06:41:30 d2.utils.events]: \u001b[0m eta: 2:51:23  iter: 3099  total_loss: 0.4177  loss_cls: 0.07621  loss_box_reg: 0.165  loss_mask: 0.1456  loss_rpn_cls: 0.003186  loss_rpn_loc: 0.0296    time: 11.6492  last_time: 11.6504  data_time: 0.0031  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 06:45:17 d2.utils.events]: \u001b[0m eta: 2:47:39  iter: 3119  total_loss: 0.4124  loss_cls: 0.07383  loss_box_reg: 0.1626  loss_mask: 0.1441  loss_rpn_cls: 0.00261  loss_rpn_loc: 0.02875    time: 11.6473  last_time: 11.4218  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 06:49:02 d2.utils.events]: \u001b[0m eta: 2:43:50  iter: 3139  total_loss: 0.3994  loss_cls: 0.07073  loss_box_reg: 0.1539  loss_mask: 0.1343  loss_rpn_cls: 0.002111  loss_rpn_loc: 0.02705    time: 11.6447  last_time: 11.4050  data_time: 0.0030  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 06:52:50 d2.utils.events]: \u001b[0m eta: 2:40:03  iter: 3159  total_loss: 0.4487  loss_cls: 0.07593  loss_box_reg: 0.1782  loss_mask: 0.1488  loss_rpn_cls: 0.002369  loss_rpn_loc: 0.03235    time: 11.6433  last_time: 11.4486  data_time: 0.0029  last_data_time: 0.0031   lr: 0.001  \n",
            "\u001b[32m[06/12 06:56:38 d2.utils.events]: \u001b[0m eta: 2:36:19  iter: 3179  total_loss: 0.4047  loss_cls: 0.07135  loss_box_reg: 0.1567  loss_mask: 0.1401  loss_rpn_cls: 0.002922  loss_rpn_loc: 0.02789    time: 11.6418  last_time: 11.7420  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 07:00:22 d2.utils.events]: \u001b[0m eta: 2:32:32  iter: 3199  total_loss: 0.429  loss_cls: 0.08371  loss_box_reg: 0.1668  loss_mask: 0.1429  loss_rpn_cls: 0.003343  loss_rpn_loc: 0.03022    time: 11.6388  last_time: 10.1565  data_time: 0.0031  last_data_time: 0.0036   lr: 0.001  \n",
            "\u001b[32m[06/12 07:04:07 d2.utils.events]: \u001b[0m eta: 2:28:43  iter: 3219  total_loss: 0.4139  loss_cls: 0.07366  loss_box_reg: 0.1615  loss_mask: 0.1419  loss_rpn_cls: 0.003216  loss_rpn_loc: 0.02808    time: 11.6365  last_time: 11.5367  data_time: 0.0029  last_data_time: 0.0033   lr: 0.001  \n",
            "\u001b[32m[06/12 07:07:53 d2.utils.events]: \u001b[0m eta: 2:24:54  iter: 3239  total_loss: 0.3984  loss_cls: 0.06845  loss_box_reg: 0.1546  loss_mask: 0.1446  loss_rpn_cls: 0.001648  loss_rpn_loc: 0.02741    time: 11.6342  last_time: 11.1523  data_time: 0.0029  last_data_time: 0.0024   lr: 0.001  \n",
            "\u001b[32m[06/12 07:11:37 d2.utils.events]: \u001b[0m eta: 2:21:00  iter: 3259  total_loss: 0.4232  loss_cls: 0.07485  loss_box_reg: 0.1636  loss_mask: 0.1504  loss_rpn_cls: 0.003373  loss_rpn_loc: 0.03181    time: 11.6316  last_time: 11.6099  data_time: 0.0031  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 07:15:24 d2.utils.events]: \u001b[0m eta: 2:17:12  iter: 3279  total_loss: 0.3913  loss_cls: 0.06559  loss_box_reg: 0.1572  loss_mask: 0.1362  loss_rpn_cls: 0.002742  loss_rpn_loc: 0.02782    time: 11.6298  last_time: 11.2473  data_time: 0.0028  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 07:19:09 d2.utils.events]: \u001b[0m eta: 2:13:24  iter: 3299  total_loss: 0.4152  loss_cls: 0.07704  loss_box_reg: 0.1627  loss_mask: 0.1422  loss_rpn_cls: 0.003201  loss_rpn_loc: 0.03159    time: 11.6278  last_time: 11.7092  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 07:22:54 d2.utils.events]: \u001b[0m eta: 2:09:36  iter: 3319  total_loss: 0.4049  loss_cls: 0.07734  loss_box_reg: 0.1486  loss_mask: 0.1402  loss_rpn_cls: 0.002413  loss_rpn_loc: 0.03105    time: 11.6253  last_time: 11.4095  data_time: 0.0030  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 07:26:44 d2.utils.events]: \u001b[0m eta: 2:05:47  iter: 3339  total_loss: 0.4301  loss_cls: 0.07722  loss_box_reg: 0.1627  loss_mask: 0.1454  loss_rpn_cls: 0.002438  loss_rpn_loc: 0.03201    time: 11.6245  last_time: 11.9043  data_time: 0.0028  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 07:30:54 d2.utils.events]: \u001b[0m eta: 2:02:02  iter: 3359  total_loss: 0.3956  loss_cls: 0.07303  loss_box_reg: 0.1552  loss_mask: 0.1361  loss_rpn_cls: 0.001939  loss_rpn_loc: 0.02763    time: 11.6297  last_time: 12.0127  data_time: 0.0036  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 07:34:59 d2.utils.events]: \u001b[0m eta: 1:58:16  iter: 3379  total_loss: 0.4109  loss_cls: 0.0792  loss_box_reg: 0.1598  loss_mask: 0.1382  loss_rpn_cls: 0.002273  loss_rpn_loc: 0.0305    time: 11.6336  last_time: 12.0216  data_time: 0.0031  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 07:38:52 d2.utils.events]: \u001b[0m eta: 1:54:27  iter: 3399  total_loss: 0.4055  loss_cls: 0.07301  loss_box_reg: 0.1577  loss_mask: 0.1365  loss_rpn_cls: 0.003322  loss_rpn_loc: 0.03091    time: 11.6337  last_time: 11.8099  data_time: 0.0029  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 07:42:50 d2.utils.events]: \u001b[0m eta: 1:50:40  iter: 3419  total_loss: 0.4068  loss_cls: 0.07177  loss_box_reg: 0.1569  loss_mask: 0.1427  loss_rpn_cls: 0.002967  loss_rpn_loc: 0.02875    time: 11.6351  last_time: 12.0250  data_time: 0.0031  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 07:46:48 d2.utils.events]: \u001b[0m eta: 1:46:55  iter: 3439  total_loss: 0.3835  loss_cls: 0.06349  loss_box_reg: 0.1452  loss_mask: 0.1365  loss_rpn_cls: 0.002229  loss_rpn_loc: 0.02658    time: 11.6367  last_time: 11.9342  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 07:50:41 d2.utils.events]: \u001b[0m eta: 1:43:08  iter: 3459  total_loss: 0.4158  loss_cls: 0.07696  loss_box_reg: 0.1606  loss_mask: 0.1427  loss_rpn_cls: 0.002464  loss_rpn_loc: 0.03215    time: 11.6369  last_time: 11.7850  data_time: 0.0029  last_data_time: 0.0037   lr: 0.001  \n",
            "\u001b[32m[06/12 07:54:34 d2.utils.events]: \u001b[0m eta: 1:39:19  iter: 3479  total_loss: 0.4301  loss_cls: 0.07604  loss_box_reg: 0.166  loss_mask: 0.1409  loss_rpn_cls: 0.003005  loss_rpn_loc: 0.03237    time: 11.6370  last_time: 11.2984  data_time: 0.0031  last_data_time: 0.0030   lr: 0.001  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/12 07:59:30 d2.utils.events]: \u001b[0m eta: 1:35:34  iter: 3499  total_loss: 0.398  loss_cls: 0.07112  loss_box_reg: 0.1504  loss_mask: 0.1375  loss_rpn_cls: 0.002687  loss_rpn_loc: 0.0297    time: 11.6551  last_time: 21.4810  data_time: 0.0044  last_data_time: 0.0065   lr: 0.001  \n",
            "\u001b[32m[06/12 08:04:59 d2.utils.events]: \u001b[0m eta: 1:31:47  iter: 3519  total_loss: 0.3934  loss_cls: 0.06575  loss_box_reg: 0.151  loss_mask: 0.1363  loss_rpn_cls: 0.002342  loss_rpn_loc: 0.02749    time: 11.6821  last_time: 11.9398  data_time: 0.0048  last_data_time: 0.0042   lr: 0.001  \n",
            "\u001b[32m[06/12 08:08:52 d2.utils.events]: \u001b[0m eta: 1:28:03  iter: 3539  total_loss: 0.3806  loss_cls: 0.0703  loss_box_reg: 0.1501  loss_mask: 0.1357  loss_rpn_cls: 0.002731  loss_rpn_loc: 0.0267    time: 11.6820  last_time: 11.8542  data_time: 0.0029  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 08:13:01 d2.utils.events]: \u001b[0m eta: 1:24:14  iter: 3559  total_loss: 0.398  loss_cls: 0.07124  loss_box_reg: 0.1514  loss_mask: 0.1415  loss_rpn_cls: 0.001892  loss_rpn_loc: 0.03045    time: 11.6862  last_time: 13.3392  data_time: 0.0031  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 08:17:25 d2.utils.events]: \u001b[0m eta: 1:20:28  iter: 3579  total_loss: 0.4047  loss_cls: 0.07905  loss_box_reg: 0.1537  loss_mask: 0.1346  loss_rpn_cls: 0.002533  loss_rpn_loc: 0.02898    time: 11.6947  last_time: 13.2255  data_time: 0.0035  last_data_time: 0.0033   lr: 0.001  \n",
            "\u001b[32m[06/12 08:21:22 d2.utils.events]: \u001b[0m eta: 1:16:42  iter: 3599  total_loss: 0.4146  loss_cls: 0.07775  loss_box_reg: 0.1535  loss_mask: 0.137  loss_rpn_cls: 0.002215  loss_rpn_loc: 0.03156    time: 11.6958  last_time: 11.5996  data_time: 0.0030  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 08:25:13 d2.utils.events]: \u001b[0m eta: 1:12:54  iter: 3619  total_loss: 0.398  loss_cls: 0.0727  loss_box_reg: 0.1503  loss_mask: 0.1378  loss_rpn_cls: 0.002932  loss_rpn_loc: 0.02833    time: 11.6948  last_time: 11.0536  data_time: 0.0029  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 08:29:02 d2.utils.events]: \u001b[0m eta: 1:09:06  iter: 3639  total_loss: 0.4074  loss_cls: 0.07513  loss_box_reg: 0.1566  loss_mask: 0.1409  loss_rpn_cls: 0.001942  loss_rpn_loc: 0.02894    time: 11.6936  last_time: 11.9640  data_time: 0.0029  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 08:32:56 d2.utils.events]: \u001b[0m eta: 1:05:18  iter: 3659  total_loss: 0.3813  loss_cls: 0.06609  loss_box_reg: 0.1443  loss_mask: 0.1344  loss_rpn_cls: 0.002392  loss_rpn_loc: 0.02676    time: 11.6935  last_time: 12.0536  data_time: 0.0029  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 08:37:17 d2.utils.events]: \u001b[0m eta: 1:01:30  iter: 3679  total_loss: 0.3704  loss_cls: 0.06808  loss_box_reg: 0.1434  loss_mask: 0.1361  loss_rpn_cls: 0.002412  loss_rpn_loc: 0.02689    time: 11.7008  last_time: 11.0132  data_time: 0.0035  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 08:41:11 d2.utils.events]: \u001b[0m eta: 0:57:41  iter: 3699  total_loss: 0.3823  loss_cls: 0.06671  loss_box_reg: 0.1439  loss_mask: 0.1317  loss_rpn_cls: 0.001579  loss_rpn_loc: 0.02637    time: 11.7009  last_time: 10.9681  data_time: 0.0030  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 08:45:03 d2.utils.events]: \u001b[0m eta: 0:53:52  iter: 3719  total_loss: 0.3449  loss_cls: 0.06205  loss_box_reg: 0.1384  loss_mask: 0.1261  loss_rpn_cls: 0.001716  loss_rpn_loc: 0.02212    time: 11.7004  last_time: 11.9294  data_time: 0.0031  last_data_time: 0.0029   lr: 0.001  \n",
            "\u001b[32m[06/12 08:48:54 d2.utils.events]: \u001b[0m eta: 0:50:04  iter: 3739  total_loss: 0.3915  loss_cls: 0.06982  loss_box_reg: 0.1542  loss_mask: 0.1397  loss_rpn_cls: 0.002015  loss_rpn_loc: 0.02955    time: 11.6997  last_time: 11.7477  data_time: 0.0032  last_data_time: 0.0032   lr: 0.001  \n",
            "\u001b[32m[06/12 08:52:45 d2.utils.events]: \u001b[0m eta: 0:46:14  iter: 3759  total_loss: 0.3944  loss_cls: 0.07101  loss_box_reg: 0.1463  loss_mask: 0.1391  loss_rpn_cls: 0.001939  loss_rpn_loc: 0.02837    time: 11.6988  last_time: 11.8938  data_time: 0.0029  last_data_time: 0.0034   lr: 0.001  \n",
            "\u001b[32m[06/12 08:56:47 d2.utils.events]: \u001b[0m eta: 0:42:26  iter: 3779  total_loss: 0.3882  loss_cls: 0.07176  loss_box_reg: 0.1455  loss_mask: 0.1333  loss_rpn_cls: 0.001764  loss_rpn_loc: 0.03045    time: 11.7008  last_time: 12.1022  data_time: 0.0029  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 09:00:50 d2.utils.events]: \u001b[0m eta: 0:38:36  iter: 3799  total_loss: 0.3894  loss_cls: 0.07093  loss_box_reg: 0.142  loss_mask: 0.134  loss_rpn_cls: 0.002445  loss_rpn_loc: 0.02921    time: 11.7031  last_time: 11.6384  data_time: 0.0031  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 09:04:58 d2.utils.events]: \u001b[0m eta: 0:34:48  iter: 3819  total_loss: 0.3841  loss_cls: 0.06697  loss_box_reg: 0.1497  loss_mask: 0.133  loss_rpn_cls: 0.002083  loss_rpn_loc: 0.02914    time: 11.7069  last_time: 12.5117  data_time: 0.0033  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 09:09:08 d2.utils.events]: \u001b[0m eta: 0:30:59  iter: 3839  total_loss: 0.3725  loss_cls: 0.06581  loss_box_reg: 0.1389  loss_mask: 0.1325  loss_rpn_cls: 0.002949  loss_rpn_loc: 0.02506    time: 11.7111  last_time: 12.6864  data_time: 0.0033  last_data_time: 0.0035   lr: 0.001  \n",
            "\u001b[32m[06/12 09:13:21 d2.utils.events]: \u001b[0m eta: 0:27:09  iter: 3859  total_loss: 0.3968  loss_cls: 0.07379  loss_box_reg: 0.1523  loss_mask: 0.1374  loss_rpn_cls: 0.001893  loss_rpn_loc: 0.02731    time: 11.7158  last_time: 13.1535  data_time: 0.0035  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 09:17:32 d2.utils.events]: \u001b[0m eta: 0:23:19  iter: 3879  total_loss: 0.3607  loss_cls: 0.06457  loss_box_reg: 0.1407  loss_mask: 0.1292  loss_rpn_cls: 0.002094  loss_rpn_loc: 0.02533    time: 11.7202  last_time: 11.6209  data_time: 0.0034  last_data_time: 0.0027   lr: 0.001  \n",
            "\u001b[32m[06/12 09:21:44 d2.utils.events]: \u001b[0m eta: 0:19:28  iter: 3899  total_loss: 0.3593  loss_cls: 0.05941  loss_box_reg: 0.1421  loss_mask: 0.1328  loss_rpn_cls: 0.001751  loss_rpn_loc: 0.02551    time: 11.7246  last_time: 12.8756  data_time: 0.0033  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 09:25:58 d2.utils.events]: \u001b[0m eta: 0:15:36  iter: 3919  total_loss: 0.3911  loss_cls: 0.06941  loss_box_reg: 0.1493  loss_mask: 0.134  loss_rpn_cls: 0.001954  loss_rpn_loc: 0.03192    time: 11.7296  last_time: 13.4474  data_time: 0.0035  last_data_time: 0.0038   lr: 0.001  \n",
            "\u001b[32m[06/12 09:30:12 d2.utils.events]: \u001b[0m eta: 0:11:43  iter: 3939  total_loss: 0.3626  loss_cls: 0.06201  loss_box_reg: 0.1358  loss_mask: 0.1288  loss_rpn_cls: 0.001812  loss_rpn_loc: 0.02527    time: 11.7346  last_time: 12.3857  data_time: 0.0034  last_data_time: 0.0026   lr: 0.001  \n",
            "\u001b[32m[06/12 09:34:19 d2.utils.events]: \u001b[0m eta: 0:07:50  iter: 3959  total_loss: 0.3597  loss_cls: 0.06162  loss_box_reg: 0.1395  loss_mask: 0.1271  loss_rpn_cls: 0.002065  loss_rpn_loc: 0.02654    time: 11.7378  last_time: 12.4337  data_time: 0.0030  last_data_time: 0.0030   lr: 0.001  \n",
            "\u001b[32m[06/12 09:38:25 d2.utils.events]: \u001b[0m eta: 0:03:55  iter: 3979  total_loss: 0.3509  loss_cls: 0.06034  loss_box_reg: 0.1395  loss_mask: 0.1307  loss_rpn_cls: 0.00198  loss_rpn_loc: 0.02764    time: 11.7405  last_time: 12.3880  data_time: 0.0032  last_data_time: 0.0028   lr: 0.001  \n",
            "\u001b[32m[06/12 09:42:32 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 3999  total_loss: 0.3887  loss_cls: 0.0745  loss_box_reg: 0.1496  loss_mask: 0.1341  loss_rpn_cls: 0.002289  loss_rpn_loc: 0.03059    time: 11.7434  last_time: 12.7798  data_time: 0.0031  last_data_time: 0.0025   lr: 0.001  \n",
            "\u001b[32m[06/12 09:42:32 d2.engine.hooks]: \u001b[0mOverall training speed: 3998 iterations in 13:02:30 (11.7434 s / it)\n",
            "\u001b[32m[06/12 09:42:32 d2.engine.hooks]: \u001b[0mTotal training time: 13:02:31 (0:00:01 on hooks)\n"
          ]
        }
      ],
      "source": [
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "#specify the path to your image and json annotation file (COCO)\n",
        "register_coco_instances(\"satellite_dataset\", {}, \"./labeled_sat_images/result.json\", \"./labeled_sat_images/images\")\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "#choose a model from model zoo\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"satellite_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  #para testes\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8 #Batch Size\n",
        "cfg.SOLVER.BASE_LR = 0.001  #Learning Rate  (menor por causa do Adam)\n",
        "cfg.SOLVER.MAX_ITER = 4000  #Number of iterations\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  #my classes: rooftop and solar\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "#set optimizer to Adam\n",
        "cfg.SOLVER.NAME = \"ADAM\"\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "#create a Trainer and starts training!\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97b3df5",
      "metadata": {
        "id": "d97b3df5"
      },
      "source": [
        "Uma outra tecninca bastatne interessante e que seria muito util para nosso projeto é a _data augmentation_. É uma técnica usada em _machine learning_ para aumentar a quantidade de dados úteis para o treinamento de modelos. Isso é feito através da aplicação de várias transformações que não alteram o significado dos dados originais.\n",
        "\n",
        "Por exemplo, com as imagens via satélite que estamos usando, algumas técnicas comuns de _data augmentation_ podem incluir rotação, escala, espelhamento, _crop_, alterações de cor, brilho, contraste, etc. Em cada uma dessas alterações, ainda estamos olhando para a mesma imagem original, mas de uma forma ligeiramente diferente.\n",
        "\n",
        "O objetivo do aumento de dados é tornar o modelo mais robusto para variações nos dados de entrada, permitindo que ele generalize melhor a partir do conjunto de treinamento para dados não vistos. Além disso, ajuda a evitar _overfitting_, que ocorre quando um modelo se ajusta muito bem ao conjunto de treinamento, mas não consegue generalizar bem para novos dados.\n",
        "\n",
        "Essa é uma tecninca que teria cedo bastatne útil para nós se tivessemos descoberto mais cedo, pois ela resolve o problema que tinhamos de tempo para trabalhar em expandir nosso _dataset_, dessa forma iriamos ter de forma artifical um aumento em nossos dados e por consequencia um modelo mais robusto. Mais infelizmente por falta de tempo não conseguimos chegar a treinar e testar esse modelo. Mas o codigo que estavamos desenvolvendo para isso se encotra abaixo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea700fb",
      "metadata": {
        "id": "9ea700fb",
        "outputId": "56fe2b2b-b281-4b70-cb36-97a34cd4048f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'DefaultPredictor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mROI_HEADS\u001b[38;5;241m.\u001b[39mSCORE_THRESH_TEST \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create predictor\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultPredictor\u001b[49m(cfg)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the image\u001b[39;00m\n\u001b[1;32m     16\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI3M30bNguq.png\u001b[39m\u001b[38;5;124m'\u001b[39m \n",
            "\u001b[0;31mNameError\u001b[0m: name 'DefaultPredictor' is not defined"
          ]
        }
      ],
      "source": [
        "from detectron2.data import DatasetMapper, build_detection_train_loader\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "\n",
        "register_coco_instances(\"satellite_dataset\", {}, \"./labeled_sat_images/result.json\", \"./labeled_sat_images/images\")\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"satellite_dataset\",)\n",
        "cfg.DATASETS.TEST = ()  #para testes\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8 #Batch Size\n",
        "cfg.SOLVER.BASE_LR = 0.001  #Learning Rate  (menor por causa do Adam)\n",
        "cfg.SOLVER.MAX_ITER = 4000  #Number of iterations\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  #my classes: rooftop and solar\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "\n",
        "#set optimizer to Adam\n",
        "cfg.SOLVER.NAME = \"ADAM\"\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "#data augmentation\n",
        "class Trainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        mapper = DatasetMapper(cfg, is_train=True, augmentations=[\n",
        "            T.ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'),\n",
        "            T.RandomFlip(horizontal=True, vertical=False),\n",
        "            T.RandomRotation(angle=[90, 90], expand=False),\n",
        "            T.RandomBrightness(0.8, 1.8),\n",
        "            T.RandomContrast(0.6, 1.3),\n",
        "            T.RandomSaturation(0.8, 1.4),\n",
        "            T.RandomLighting(0.7),\n",
        "        ])\n",
        "        return build_detection_train_loader(cfg, mapper=mapper)\n",
        "\n",
        "#use the custom trainer\n",
        "trainer = Trainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}